[{"categories":null,"content":"概述 cache.Reflector 可以说是k8s最重要的组件，它串联起k8s的整个流程。 在服务端(apiserver) ，使用 reflector 向 etcd 获取资源数据。 在连接端(informer、kubelet …)，使用 reflector 向 apiserver 获取资源数据。 k8s的整个逻辑流程中，所有这些获取资源数据相关的操作，都封装在 reflector 里面，可以看出 reflector 对于理解 k8s 的重要性。 ","date":"2023-01-14","objectID":"/2023-01-14-client-go-3-reflector/:1:0","tags":["client-go","cache.Reflector"],"title":"client-go解析(3) - cache.Reflector","uri":"/2023-01-14-client-go-3-reflector/"},{"categories":null,"content":"定义 // Reflector watches a specified resource and causes all changes to be reflected in the given store. type Reflector struct { // name identifies this reflector. By default it will be a file:line if possible. name string // The name of the type we expect to place in the store. The name // will be the stringification of expectedGVK if provided, and the // stringification of expectedType otherwise. It is for display // only, and should not be used for parsing or comparison. typeDescription string // An example object of the type we expect to place in the store. // Only the type needs to be right, except that when that is // `unstructured.Unstructured` the object's `\"apiVersion\"` and // `\"kind\"` must also be right. expectedType reflect.Type // The GVK of the object we expect to place in the store if unstructured. expectedGVK *schema.GroupVersionKind // The destination to sync up with the watch source store Store // listerWatcher is used to perform lists and watches. listerWatcher ListerWatcher // backoff manages backoff of ListWatch backoffManager wait.BackoffManager // initConnBackoffManager manages backoff the initial connection with the Watch call of ListAndWatch. initConnBackoffManager wait.BackoffManager // MaxInternalErrorRetryDuration defines how long we should retry internal errors returned by watch. MaxInternalErrorRetryDuration time.Duration resyncPeriod time.Duration // ShouldResync is invoked periodically and whenever it returns `true` the Store's Resync operation is invoked ShouldResync func() bool // clock allows tests to manipulate time clock clock.Clock // paginatedResult defines whether pagination should be forced for list calls. // It is set based on the result of the initial list call. paginatedResult bool // lastSyncResourceVersion is the resource version token last // observed when doing a sync with the underlying store // it is thread safe, but not synchronized with the underlying store lastSyncResourceVersion string // isLastSyncResourceVersionUnavailable is true if the previous list or watch request with // lastSyncResourceVersion failed with an \"expired\" or \"too large resource version\" error. isLastSyncResourceVersionUnavailable bool // lastSyncResourceVersionMutex guards read/write access to lastSyncResourceVersion lastSyncResourceVersionMutex sync.RWMutex // WatchListPageSize is the requested chunk size of initial and resync watch lists. // If unset, for consistent reads (RV=\"\") or reads that opt-into arbitrarily old data // (RV=\"0\") it will default to pager.PageSize, for the rest (RV != \"\" \u0026\u0026 RV != \"0\") // it will turn off pagination to allow serving them from watch cache. // NOTE: It should be used carefully as paginated lists are always served directly from // etcd, which is significantly less efficient and may lead to serious performance and // scalability problems. WatchListPageSize int64 // Called whenever the ListAndWatch drops the connection with an error. watchErrorHandler WatchErrorHandler } ","date":"2023-01-14","objectID":"/2023-01-14-client-go-3-reflector/:2:0","tags":["client-go","cache.Reflector"],"title":"client-go解析(3) - cache.Reflector","uri":"/2023-01-14-client-go-3-reflector/"},{"categories":null,"content":"概述 如果把k8s当成资源管理系统, 那cache.Store无疑是最核心的接口, 用于缓存,存储资源 。 reflector 依赖于 cache.Store 的实现做存储，根据不同的实现有不同的功能。所以，正确的理解cache.Store的不同实现，是理解 reflector 的关键。 ","date":"2023-01-12","objectID":"/2023-01-12-client-go-2-cache-store/:1:0","tags":["client-go","cache.Store"],"title":"client-go解析(2) - cache.Store","uri":"/2023-01-12-client-go-2-cache-store/"},{"categories":null,"content":"定义 接口定义如下： // client-go/tools/cache/store.go type Store interface { Add(obj interface{}) error Update(obj interface{}) error Delete(obj interface{}) error List() []interface{} ListKeys() []string Get(obj interface{}) (item interface{}, exists bool, err error) GetByKey(key string) (item interface{}, exists bool, err error) Replace([]interface{}, string) error Resync() error } 先看一下cache.Store的接口与实现类: client-go中: client-go k8s中: k8s 注意 在k8s中添加了两个实现类: cacheStore 和 watchCache,分别用于kubelet和apiserver,后续再作分析 接口 Indexer 和 Queue,分别在 cache.Store 的基础上,拓展了一些功能 indexer的实现类只有一个 indexer Queue的实现类有2个 Queue 注意 在client-go中,还存在一些 workqueue,在client-go/util/workqueue 包中,大致如下: workqueue 这里注意区分,不要混淆 除此之外, 直接实现cache.Store的还有 ExpirationCache, UndeltaStore, cacheStore, watchCache, UndeltaStore 是用于kubelet 获取pod数据用到的，watchCache 是apiserver缓存etcd数据据用到的。 ","date":"2023-01-12","objectID":"/2023-01-12-client-go-2-cache-store/:2:0","tags":["client-go","cache.Store"],"title":"client-go解析(2) - cache.Store","uri":"/2023-01-12-client-go-2-cache-store/"},{"categories":null,"content":"实现 由上面的分析, 这里将 cache.Store的实现分为3组. ","date":"2023-01-12","objectID":"/2023-01-12-client-go-2-cache-store/:3:0","tags":["client-go","cache.Store"],"title":"client-go解析(2) - cache.Store","uri":"/2023-01-12-client-go-2-cache-store/"},{"categories":null,"content":"基础 (cache) 在store.go文件中，就包含了 cache.Store的默认实现 cache，同时也是接口Indexer的实现类。 Indexer 接口主要是在 Store 接口的基础上拓展了对象的检索功能： type Indexer interface { Store Index(indexName string, obj interface{}) ([]interface{}, error) // 根据索引名和给定的对象返回符合条件的所有对象 IndexKeys(indexName, indexedValue string) ([]string, error) // 根据索引名和索引值返回符合条件的所有对象的 key ListIndexFuncValues(indexName string) []string // 列出索引函数计算出来的所有索引值 ByIndex(indexName, indexedValue string) ([]interface{}, error) // 根据索引名和索引值返回符合条件的所有对象 GetIndexers() Indexers // 获取所有的 Indexers，对应 map[string]IndexFunc 类型 AddIndexers(newIndexers Indexers) error // 这个方法要在数据加入存储前调用，添加更多的索引方法，默认只通过 namespace 检索 } Indexer 的默认实现是 cache： type cache struct { cacheStorage ThreadSafeStore keyFunc KeyFunc } 相关构造器: // NewStore returns a Store implemented simply with a map and a lock. func NewStore(keyFunc KeyFunc) Store { return \u0026cache{ cacheStorage: NewThreadSafeStore(Indexers{}, Indices{}), keyFunc: keyFunc, } } // NewIndexer returns an Indexer implemented simply with a map and a lock. func NewIndexer(keyFunc KeyFunc, indexers Indexers) Indexer { return \u0026cache{ cacheStorage: NewThreadSafeStore(indexers, Indices{}), keyFunc: keyFunc, } } 由注释可知，基础的Store，由一个map和lock实现。 根据是否传入 indexers 索引函数，判断是否开启索引功能。 可以从测试用例中,了解他们的用法 // 数据类型 type testStoreObject struct { id string val string } // 获取key的函数,使用 id 作为Key func testStoreKeyFunc(obj interface{}) (string, error) { return obj.(testStoreObject).id, nil } Store func TestCache(t *testing.T) { doTestStore(t, NewStore(testStoreKeyFunc)) } Store 添加一个元素之后, Store Store实现了基本的crud功能, index 为空. Indexer func TestIndex(t *testing.T) { doTestIndex(t, NewIndexer(testStoreKeyFunc, testStoreIndexers())) } 与构造Store相比, 构造Indexer多传进了一个 Indexers map,如下: func testStoreIndexers() Indexers { indexers := Indexers{} indexers[\"by_val\"] = testStoreIndexFunc return indexers } func testStoreIndexFunc(obj interface{}) ([]string, error) { return []string{obj.(testStoreObject).val}, nil } 该函数定义了一个索引 “by_val”, 具体是使用对象的 val 作为索引值. Add() // Add inserts an item into the cache. func (c *cache) Add(obj interface{}) error { key, err := c.keyFunc(obj) if err != nil { return KeyError{obj, err} } c.cacheStorage.Add(key, obj) return nil } func (c *threadSafeMap) Add(key string, obj interface{}) { c.Update(key, obj) } func (c *threadSafeMap) Update(key string, obj interface{}) { c.lock.Lock() defer c.lock.Unlock() oldObject := c.items[key] c.items[key] = obj c.index.updateIndices(oldObject, obj, key) } 添加元素后, indexer内容如下: indexer.Add(mkObj(\"a\", \"b\")) indexer.Add(mkObj(\"c\", \"b\")) indexer.Add(mkObj(\"e\", \"f\")) indexer.Add(mkObj(\"g\", \"h\")) Indexer 与 Store 相比, index 不为空. indexers 存储了索引函数 indices 存储了具体的索引数据 update() // Update sets an item in the cache to its updated state. func (c *cache) Update(obj interface{}) error { key, err := c.keyFunc(obj) if err != nil { return KeyError{obj, err} } c.cacheStorage.Update(key, obj) return nil } delete() // Delete removes an item from the cache. func (c *cache) Delete(obj interface{}) error { key, err := c.keyFunc(obj) if err != nil { return KeyError{obj, err} } c.cacheStorage.Delete(key) return nil } func (c *threadSafeMap) Delete(key string) { c.lock.Lock() defer c.lock.Unlock() if obj, exists := c.items[key]; exists { c.index.updateIndices(obj, nil, key) delete(c.items, key) } } ","date":"2023-01-12","objectID":"/2023-01-12-client-go-2-cache-store/:3:1","tags":["client-go","cache.Store"],"title":"client-go解析(2) - cache.Store","uri":"/2023-01-12-client-go-2-cache-store/"},{"categories":null,"content":"队列 队列接口定义如下 type Queue interface { Store Pop(PopProcessFunc) (interface{}, error) AddIfNotPresent(interface{}) error HasSynced() bool Close() } Queue是在Store基础上扩展了Pop接口可以让对象有序的弹出 FIFO // FIFO is a Queue in which (a) each accumulator is simply the most // recently provided object and (b) the collection of keys to process // is a FIFO. The accumulators all start out empty, and deleting an // object from its accumulator empties the accumulator. The Resync // operation is a no-op. // // Thus: if multiple adds/updates of a single object happen while that // object's key is in the queue before it has been processed then it // will only be processed once, and when it is processed the most // recent version will be processed. This can't be done with a channel // // FIFO solves this use case: // - You want to process every object (exactly) once. // - You want to process the most recent version of the object when you process it. // - You do not want to process deleted objects, they should be removed from the queue. // - You do not want to periodically reprocess objects. // // Compare with DeltaFIFO for other use cases. type FIFO struct { lock sync.RWMutex cond sync.Cond // We depend on the property that every key in `items` is also in `queue` items map[string]interface{} queue []string // populated is true if the first batch of items inserted by Replace() has been populated // or Delete/Add/Update was called first. populated bool // initialPopulationCount is the number of items inserted by the first call of Replace() initialPopulationCount int // keyFunc is used to make the key used for queued item insertion and retrieval, and // should be deterministic. keyFunc KeyFunc // Indication the queue is closed. // Used to indicate a queue is closed so a control loop can exit when a queue is empty. // Currently, not used to gate any of CRUD operations. closed bool } 构造函数: // NewFIFO returns a Store which can be used to queue up items to // process. func NewFIFO(keyFunc KeyFunc) *FIFO { f := \u0026FIFO{ items: map[string]interface{}{}, queue: []string{}, keyFunc: keyFunc, } f.cond.L = \u0026f.lock return f } 关于FIFO, 需要注意的是,如果pop处理失败,会重新放入队列 FIFO在client-go/k8s中并不多见,重点是下面的 DeltaFIFO DeltaFIFO informer 模式中，Reflector 使用 DeltaFIFO 作为 store，向 apiserver 中获取数据 定义: // DeltaFIFO is a producer-consumer queue, where a Reflector is // intended to be the producer, and the consumer is whatever calls // the Pop() method. // 将 DeltaFIFO 当成一个生产-消费队列 type DeltaFIFO struct { lock sync.RWMutex cond sync.Cond items map[string]Deltas queue []string populated bool initialPopulationCount int keyFunc KeyFunc knownObjects KeyListerGetter closed bool emitDeltaTypeReplaced bool } Add() / Update() / Delete() add() / update() / delete() 可以视为生产者，生产类型为 Added / Updated / Deleted 的 Delta 数据 func (f *DeltaFIFO) Add(obj interface{}) error { f.lock.Lock() defer f.lock.Unlock() f.populated = true return f.queueActionLocked(Added, obj) } func (f *DeltaFIFO) Update(obj interface{}) error { f.lock.Lock() defer f.lock.Unlock() f.populated = true return f.queueActionLocked(Updated, obj) } func (f *DeltaFIFO) Delete(obj interface{}) error { id, err := f.KeyOf(obj) if err != nil { return KeyError{obj, err} } f.lock.Lock() defer f.lock.Unlock() f.populated = true if f.knownObjects == nil { if _, exists := f.items[id]; !exists { return nil } } else { _, exists, err := f.knownObjects.GetByKey(id) _, itemsExist := f.items[id] if err == nil \u0026\u0026 !exists \u0026\u0026 !itemsExist { return nil } } return f.queueActionLocked(Deleted, obj) } func (f *DeltaFIFO) queueActionLocked(actionType DeltaType, obj interface{}) error { id, err := f.KeyOf(obj) if err != nil { return KeyError{obj, err} } oldDeltas := f.items[id] newDeltas := append(oldDeltas, Delta{actionType, obj}) newDeltas = dedupDeltas(newDeltas) if len(newDeltas) \u003e 0 { if _, exists := f.items[id]; !exists { f.queue = append(f.queue, id) } f.items[id] = newDeltas f.cond.Broadcast() } else { if oldDeltas == nil { klog.Errorf(\"Impossible dedupDeltas for id=%q: oldDel","date":"2023-01-12","objectID":"/2023-01-12-client-go-2-cache-store/:3:2","tags":["client-go","cache.Store"],"title":"client-go解析(2) - cache.Store","uri":"/2023-01-12-client-go-2-cache-store/"},{"categories":null,"content":"组合 UndeltaStore kubelet 中，Reflector 使用 UndeltaStore 作为 store，向 apiserver 中获取数据 watchCache apiserver 中，Reflector 使用 watchCache 作为 store，向 etcd 中获取数据 ","date":"2023-01-12","objectID":"/2023-01-12-client-go-2-cache-store/:3:3","tags":["client-go","cache.Store"],"title":"client-go解析(2) - cache.Store","uri":"/2023-01-12-client-go-2-cache-store/"},{"categories":null,"content":"概述 k8s许多重要的基础类型，都在client-go中定义。 如 cache.Store、cache.Reflector等。 接下来分析这些重要的接口和类型，以及他们的组合(比如informer)在k8s中的使用。 ","date":"2023-01-12","objectID":"/2023-01-12-client-go-1/:1:0","tags":["client-go"],"title":"client-go解析(1)","uri":"/2023-01-12-client-go-1/"},{"categories":null,"content":"使用 client-go的一些用法，可以参考 https://github.com/iximiuz/client-go-examples ","date":"2023-01-12","objectID":"/2023-01-12-client-go-1/:2:0","tags":["client-go"],"title":"client-go解析(1)","uri":"/2023-01-12-client-go-1/"},{"categories":null,"content":"概述 分析containerd网络，从不同的上层应用： nerdctl run -d –name runcdev -p 8080:80 nginx kubectl run nginx –image=nginx 根据containerd 的 Scope and principles The following table specifies the various components of containerd and general features of container runtimes. The table specifies whether or not the feature/component is in or out of scope. Name Description In/Out Reason execution Provide an extensible execution layer for executing a container in Create,start, stop pause, resume exec, signal, delete cow filesystem Built in functionality for overlay, aufs, and other copy on write filesystems for containers in distribution Having the ability to push and pull images as well as operations on images as a first class API object in containerd will fully support the management and retrieval of images metrics container-level metrics, cgroup stats, and OOM events in networking creation and management of network interfaces out Networking will be handled and provided to containerd via higher level systems. build Building images as a first class API out Build is a higher level tooling feature and can be implemented in many different ways on top of containerd volumes Volume management for external data out The API supports mounts, binds, etc where all volumes type systems can be built on top of containerd. logging Persisting container logs out Logging can be build on top of containerd because the container’s STDIO will be provided to the clients and they can persist any way they see fit. There is no io copying of container STDIO in containerd. 从上表可知，containerd并不负责容器网络的配置工作。 ","date":"2022-12-02","objectID":"/2022-12-02-containerd-12-containerd-network/:1:0","tags":["containerd","network","hook"],"title":"Containerd解析(12) network","uri":"/2022-12-02-containerd-12-containerd-network/"},{"categories":null,"content":"nerdctl run -d –name runcdev -p 8080:80 nginx nerdctl使用cri插件进行网络配置，默认插件为bridge 从一个报错开始 删除 cni bridge二进制文件，sudo rm -rf /opt/cni/bin/bridge，重新创建容器，报错如下： [xiu-notebook] ~ n run -d --name runcdev -p 8080:80 nginx [sudo] password for xiu: FATA[0005] failed to create shim task: OCI runtime create failed: container_linux.go:380: starting container process caused: process_linux.go:545: container init caused: Running hook #0:: error running hook: exit status 1, stdout: , stderr: time=\"2022-12-02T13:58:11+08:00\" level=fatal msg=\"failed to call cni.Setup: plugin type=\\\"bridge\\\" failed (add): failed to find plugin \\\"bridge\\\" in path [/opt/cni/bin]\" Failed to write to log, write /var/lib/nerdctl/1935db59/containers/default/7aef42386e604726fec25f9f62acbf128c1b5648cd41259eefa3503602f65353/oci-hook.createRuntime.log: file already closed: unknown 查看hook相关定义：Hooks CreateRuntime Hooks The createRuntime hooks MUST be called as part of the create operation after the runtime environment has been created (according to the configuration in config.json) but before the pivot_root or any equivalent operation has been executed. The createRuntime hooks’ path MUST resolve in the runtime namespace. The createRuntime hooks MUST be executed in the runtime namespace. On Linux, for example, they are called after the container namespaces are created, so they provide an opportunity to customize the container (e.g. the network namespace could be specified in this hook). The definition of createRuntime hooks is currently underspecified and hooks authors, should only expect from the runtime that the mount namespace have been created and the mount operations performed. Other operations such as cgroups and SELinux/AppArmor labels might not have been performed by the runtime. Note: runc originally implemented prestart hooks contrary to the spec, namely as part of the create operation (instead of during the start operation). This incorrect implementation actually corresponds to createRuntime hooks. For runtimes that implement the deprecated prestart hooks as createRuntime hooks, createRuntime hooks MUST be called after the prestart hooks. hook 查看容器config.json中hook的相关内容： { \"ociVersion\": \"1.0.2-dev\", \"process\": {}, \"root\": { \"path\": \"rootfs\" }, \"hostname\": \"5bed8c875756\", \"mounts\": [], \"hooks\": { \"createRuntime\": [ { \"path\": \"/home/xiu/CloudDrive/UBUNTU/application/bin/nerdctl\", \"args\": [ \"/home/xiu/CloudDrive/UBUNTU/application/bin/nerdctl\", \"internal\", \"oci-hook\", \"createRuntime\" ], \"env\": [ ] } ], \"poststop\": [ { \"path\": \"/home/xiu/CloudDrive/UBUNTU/application/bin/nerdctl\", \"args\": [ \"/home/xiu/CloudDrive/UBUNTU/application/bin/nerdctl\", \"internal\", \"oci-hook\", \"postStop\" ], \"env\": [ ] } ] }, \"annotations\": { \"io.containerd.image.config.stop-signal\": \"SIGQUIT\", \"nerdctl/extraHosts\": \"null\", \"nerdctl/hostname\": \"5bed8c875756\", \"nerdctl/log-uri\": \"binary:///home/xiu/CloudDrive/UBUNTU/application/bin/nerdctl?_NERDCTL_INTERNAL_LOGGING=%2Fvar%2Flib%2Fnerdctl%2F1935db59\", \"nerdctl/name\": \"runcdev\", \"nerdctl/namespace\": \"default\", \"nerdctl/networks\": \"[\\\"bridge\\\"]\", \"nerdctl/platform\": \"linux/amd64\", \"nerdctl/ports\": \"[{\\\"HostPort\\\":8080,\\\"ContainerPort\\\":80,\\\"Protocol\\\":\\\"tcp\\\",\\\"HostIP\\\":\\\"0.0.0.0\\\"}]\", \"nerdctl/state-dir\": \"/var/lib/nerdctl/1935db59/containers/default/5bed8c875756abdac43e664d2e7a1950a838675ca143ab71e3a4d8bd0ef9d59f\" }, \"linux\": {} } nerdctl在创建容器过程中，会添加createRuntime，poststop两个hook，不管存不存在-p参数，构造hook的方法如下： nerdctl/cmd/nerdctl/run.go func withNerdctlOCIHook(cmd *cobra.Command, id string) (oci.SpecOpts, error) { selfExe, f := globalFlags(cmd) args := append([]string{selfExe}, append(f, \"internal\", \"oci-hook\")...) return func(_ context.Context, _ oci.Client, _ *containers.Container, s *specs.Spec) error { if s.Hooks == nil { s.Hooks = \u0026specs.Hooks{} } crArgs := append(args, \"createRuntime\") s.Hooks.CreateRuntime = append(s.Hooks.CreateRuntime, specs.Hook{ Path: selfExe, Args: crArgs, Env: os.Environ(), }) argsCopy := append([]string(nil), args...) psArgs := app","date":"2022-12-02","objectID":"/2022-12-02-containerd-12-containerd-network/:2:0","tags":["containerd","network","hook"],"title":"Containerd解析(12) network","uri":"/2022-12-02-containerd-12-containerd-network/"},{"categories":null,"content":"kubectl run nginx –image=nginx ","date":"2022-12-02","objectID":"/2022-12-02-containerd-12-containerd-network/:3:0","tags":["containerd","network","hook"],"title":"Containerd解析(12) network","uri":"/2022-12-02-containerd-12-containerd-network/"},{"categories":null,"content":"概述 在 containerd 的 event 中，主要用到了 go-events 这个包。 为 Go 实现一个可组合的事件分发包。 ","date":"2022-11-23","objectID":"/2022-11-23-containerd-11-go-events/:1:0","tags":["containerd","golang","events","package"],"title":"Containerd解析(11) - event \u0026 go-events","uri":"/2022-11-23-containerd-11-go-events/"},{"categories":null,"content":"初始化 在containerd启动过程中， // New creates and initializes a new containerd server func New(ctx context.Context, config *srvconfig.Config) (*Server, error) { ... ... var events = exchange.NewExchange() ... ... for _, p := range plugins { ... initContext := plugin.NewContext( ctx, p, initialized, config.Root, config.State, ) initContext.Events = events initContext.Address = config.GRPC.Address initContext.TTRPCAddress = config.TTRPC.Address ... ... } } 使用 exchange.NewExchange() 初始化，并将它传递个各个plugin，用于处理事件。 containerd中的events处理逻辑与shim 中的Monitor逻辑类似，在需要订阅的地方调用Subscribe，然后channel，channel会接受到对应的Envelope(也就是相关事件)。 Subscribe在cri插件中的应用比较多。 containerd中的事件分发还与grpc进行了结合，可以通过grpc接口进行事件的订阅。 grpc接口的定义： service Events { rpc Publish(PublishRequest) returns (google.protobuf.Empty); rpc Forward(ForwardRequest) returns (google.protobuf.Empty); rpc Subscribe(SubscribeRequest) returns (stream Envelope); } 服务端实现： func (s *service) Subscribe(req *api.SubscribeRequest, srv api.Events_SubscribeServer) error { ctx, cancel := context.WithCancel(srv.Context()) defer cancel() eventq, errq := s.events.Subscribe(ctx, req.Filters...) for { select { case ev := \u003c-eventq: if err := srv.Send(toProto(ev)); err != nil { return fmt.Errorf(\"failed sending event to subscriber: %w\", err) } case err := \u003c-errq: if err != nil { return fmt.Errorf(\"subscription error: %w\", err) } return nil } } } ","date":"2022-11-23","objectID":"/2022-11-23-containerd-11-go-events/:2:0","tags":["containerd","golang","events","package"],"title":"Containerd解析(11) - event \u0026 go-events","uri":"/2022-11-23-containerd-11-go-events/"},{"categories":null,"content":"概述 了解containerd的启动过程。 重点关注api接口 ContainersClient、 TasksClient的实现。 ContainersClient的实现类是： // containerd/services/containers/local.go type local struct { containers.Store db *metadata.DB publisher events.Publisher } TasksClient的实现类是： type local struct { runtimes map[string]runtime.PlatformRuntime containers containers.Store store content.Store publisher events.Publisher monitor runtime.TaskMonitor v2Runtime runtime.PlatformRuntime } ","date":"2022-11-18","objectID":"/2022-11-18-containerd-10-containerd-start/:1:0","tags":["containerd"],"title":"Containerd解析(10) - Containerd Start","uri":"/2022-11-18-containerd-10-containerd-start/"},{"categories":null,"content":"概述 参考文章，该文时间比较久远，containerd的很多逻辑已经发生变化。 学习该文思路，重新整理Containerd,Containerd-shim,runc 之间的依存关系。 ","date":"2022-11-18","objectID":"/2022-11-18-containerd-9-containerd-containerd-shim-runc/:1:0","tags":["containerd","containerd-shim","runc"],"title":"Containerd解析(9) - Containerd,Containerd-shim,runc的依存关系","uri":"/2022-11-18-containerd-9-containerd-containerd-shim-runc/"},{"categories":null,"content":"整体关系 使用nerdctl启动容器n run -d --name runcdev q946666800/runcdev 进程如下： root 3716 1 0 10:36 ? 00:00:00 /usr/bin/containerd root 4509 1 0 10:37 ? 00:00:00 /usr/bin/containerd-shim-runc-v2 root 4567 4509 0 10:37 ? 00:00:00 \\_ /main-example containerd 与 shim的父进程都是1, 容器进程父进程是shim。 接下来，分析下面四种情况： containerd进程存在的情况下，杀死containerd-shim进程； containerd进程存在的情况下，杀死容器进程； containerd进程不存在的情况下，杀死containerd-shim进程，然后启动containerd进程； containerd进程不存在的情况下，杀死容器进程，然后启动containerd进程； ","date":"2022-11-18","objectID":"/2022-11-18-containerd-9-containerd-containerd-shim-runc/:2:0","tags":["containerd","containerd-shim","runc"],"title":"Containerd解析(9) - Containerd,Containerd-shim,runc的依存关系","uri":"/2022-11-18-containerd-9-containerd-containerd-shim-runc/"},{"categories":null,"content":"第一种情况 containerd进程存在的情况下，杀死containerd-shim进程。 ~ ps -ef | egrep 'containerd|example' | grep -v grep root 3716 1 0 10:36 ? 00:00:00 /usr/bin/containerd root 4509 1 0 10:37 ? 00:00:00 /usr/bin/containerd-shim-runc-v2 root 4567 4509 0 10:37 ? 00:00:00 \\_ /main-example 使用kill -9 4509杀死shim进程，看到容器进程也跟着退出。 ~ ps -ef | egrep 'containerd|example' | grep -v grep root 3716 1 0 10:36 ? 00:00:00 /usr/bin/containerd 信息 结论：在containerd运行的情况下，杀死containerd-shim，容器进程会退出。 看下为什么容器进程会退出。 查看containerd启动containerd-shim的相关代码，也就是 m.startShim() func (m *ShimManager) startShim(ctx context.Context, bundle *Bundle, id string, opts runtime.CreateOpts) (*shim, error) { ... ... shim, err := b.Start(ctx, protobuf.FromAny(topts), func() { log.G(ctx).WithField(\"id\", id).Info(\"shim disconnected\") cleanupAfterDeadShim(context.Background(), id, ns, m.shims, m.events, b) // Remove self from the runtime task list. Even though the cleanupAfterDeadShim() // would publish taskExit event, but the shim.Delete() would always failed with ttrpc // disconnect and there is no chance to remove this dead task from runtime task lists. // Thus it's better to delete it here. m.shims.Delete(ctx, id) }) if err != nil { return nil, fmt.Errorf(\"start failed: %w\", err) } return shim, nil } 注意到，在b.Start 后面传入了一个回调函数 func() { log.G(ctx).WithField(\"id\", id).Info(\"shim disconnected\") cleanupAfterDeadShim(context.Background(), id, ns, m.shims, m.events, b) // Remove self from the runtime task list. Even though the cleanupAfterDeadShim() // would publish taskExit event, but the shim.Delete() would always failed with ttrpc // disconnect and there is no chance to remove this dead task from runtime task lists. // Thus it's better to delete it here. m.shims.Delete(ctx, id) } 从该函数名 cleanupAfterDeadShim 可以推测主要是做一些清理工作。 先看一下 b.Start() func (b *binary) Start(ctx context.Context, opts *types.Any, onClose func()) (_ *shim, err error) { args := []string{\"-id\", b.bundle.ID} switch logrus.GetLevel() { case logrus.DebugLevel, logrus.TraceLevel: args = append(args, \"-debug\") } args = append(args, \"start\") // 1. 构建cmd cmd, err := client.Command( ctx, \u0026client.CommandConfig{ Runtime: b.runtime, Address: b.containerdAddress, TTRPCAddress: b.containerdTTRPCAddress, Path: b.bundle.Path, Opts: opts, Args: args, SchedCore: b.schedCore, }) if err != nil { return nil, err } ... ... // 2. 启动shim server，并获取 sock address out, err := cmd.CombinedOutput() if err != nil { return nil, fmt.Errorf(\"%s: %w\", out, err) } address := strings.TrimSpace(string(out)) conn, err := client.Connect(address, client.AnonDialer) if err != nil { return nil, err } // 连接丢失的回调函数，包含刚刚传进来的 onClose onCloseWithShimLog := func() { onClose() cancelShimLog() f.Close() } // Save runtime binary path for restore. if err := os.WriteFile(filepath.Join(b.bundle.Path, \"shim-binary-path\"), []byte(b.runtime), 0600); err != nil { return nil, err } // 3.与sock address 建立连接，并在连接关闭的时候调用传进来的回调函数onClose client := ttrpc.NewClient(conn, ttrpc.WithOnClose(onCloseWithShimLog)) return \u0026shim{ bundle: b.bundle, client: client, }, nil } 构建cmd 启动shim server，并获取 sock address 通过sock address与刚刚启动的shim server建立连接，并在连接关闭时调用回调函数onClose，从而执行 cleanupAfterDeadShim func cleanupAfterDeadShim(ctx context.Context, id, ns string, rt *runtime.NSMap[ShimInstance], events *exchange.Exchange, binaryCall *binary) { ... response, err := binaryCall.Delete(ctx) ... } func (b *binary) Delete(ctx context.Context) (*runtime.Exit, error) { log.G(ctx).Info(\"cleaning up dead shim\") ... cmd, err := client.Command(ctx, \u0026client.CommandConfig{ Runtime: b.runtime, Address: b.containerdAddress, TTRPCAddress: b.containerdTTRPCAddress, Path: bundlePath, Opts: nil, Args: []string{ \"-id\", b.bundle.ID, \"-bundle\", b.bundle.Path, \"delete\", }, }) ... if err := cmd.Run(); err != nil { log.G(ctx).WithField(\"cmd\", cmd).WithError(err).Error(\"failed to delete\") return nil, fmt.Errorf(\"%s: %w\", errb.String(), err) } ... } 查看cmd： clean up cmd 可以看到，cleanup是通过containerd-shim的 delete 命令进行runc容器的清理，具体细节这里不进行展开。 ","date":"2022-11-18","objectID":"/2022-11-18-containerd-9-containerd-containerd-shim-runc/:3:0","tags":["containerd","containerd-shim","runc"],"title":"Containerd解析(9) - Containerd,Containerd-shim,runc的依存关系","uri":"/2022-11-18-containerd-9-containerd-containerd-shim-runc/"},{"categories":null,"content":"第二种情况 containerd进程存在的情况下，杀死容器进程; 一方面，在容器进程退出时，containerd-shim也会捕获到信号退出，这将在第四种情况下详细分析。 另一方面，容器进程退出，containerd中的monitor会会捕获到该事件，从而触发容器进程退出流程，这是本小节详细分析的内容。 杀死容器进程，shim service中的订阅者reaper.Default.Subscribe()会收到退出状态， s := \u0026service{ context: ctx, events: make(chan interface{}, 128), ec: reaper.Default.Subscribe(), ep: ep, shutdown: sd, containers: make(map[string]*runc.Container), } 然后将该退出状态推送至 containerd。 ","date":"2022-11-18","objectID":"/2022-11-18-containerd-9-containerd-containerd-shim-runc/:4:0","tags":["containerd","containerd-shim","runc"],"title":"Containerd解析(9) - Containerd,Containerd-shim,runc的依存关系","uri":"/2022-11-18-containerd-9-containerd-containerd-shim-runc/"},{"categories":null,"content":"第三种情况 containerd进程不存在的情况下，杀死containerd-shim进程，然后启动containerd进程 首先杀死 containerd 进程，shim进程和容器进程依然存在。 [xiu-desktop] ~ sp shim [shim] 相关进程信息如下： root 118803 1 0 11月27 ? 00:00:01 /usr/bin/containerd-shim-runc-v2 -namespace default -id dc236284189782b7c37131ad23473acaf8b8282e3532baf18928ec4e3949713e -address /run/containerd/containerd.sock -debug [xiu-desktop] ~ sp main-example [main-example] 相关进程信息如下： root 118880 118803 0 11月27 ? 00:00:04 /main-example 接着杀死shim进程，容器进程依然存在，并且被进程1接收（父进程变为1） [xiu-desktop] ~ kp shim [shim] 相关进程信息如下： root 118803 1 0 11月27 ? 00:00:01 /usr/bin/containerd-shim-runc-v2 -namespace default -id dc236284189782b7c37131ad23473acaf8b8282e3532baf18928ec4e3949713e -address /run/containerd/containerd.sock -debug 是否关闭 [shim] 相关进程? (y/n) y 已关闭 1 个 [shim] 相关进程 [xiu-desktop] ~ sp main-example [main-example] 相关进程信息如下： root 118880 1 0 11月27 ? 00:00:04 /main-example 然后再启动containerd，容器进程消失。 [xiu-desktop] ~ sp main-example 没有 [main-example] 相关进程 这时候容器变为 created 状态 [xiu-desktop] ~ n ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES dc2362841897 docker.io/q946666800/runcdev:latest \"/main-example\" 4 hours ago Created runcdev 代码解析： 残留容器进程的清理工作，主要在 RuntimePluginV2 插件初始化过程中触发的，相关调用堆栈如下： v2.(*binary).Delete (binary.go:154) github.com/containerd/containerd/runtime/v2 v2.cleanupAfterDeadShim (shim.go:143) github.com/containerd/containerd/runtime/v2 v2.(*ShimManager).loadShims (shim_load.go:141) github.com/containerd/containerd/runtime/v2 v2.(*ShimManager).loadExistingTasks (shim_load.go:46) github.com/containerd/containerd/runtime/v2 v2.NewShimManager (manager.go:152) github.com/containerd/containerd/runtime/v2 v2.init.0.func1 (manager.go:84) github.com/containerd/containerd/runtime/v2 plugin.(*Registration).Init (plugin.go:115) github.com/containerd/containerd/plugin server.New (server.go:230) github.com/containerd/containerd/services/server command.App.func1.1 (main.go:194) github.com/containerd/containerd/cmd/containerd/command runtime.goexit (asm_amd64.s:1571) runtime - 异步堆栈跟踪 command.App.func1 (main.go:191) github.com/containerd/containerd/cmd/containerd/command 查看几个重要的方法： loadExistingTasks func (m *ShimManager) loadExistingTasks(ctx context.Context) error { nsDirs, err := os.ReadDir(m.state) if err != nil { return err } // 遍历 ns for _, nsd := range nsDirs { if !nsd.IsDir() { continue } ns := nsd.Name() // skip hidden directories if len(ns) \u003e 0 \u0026\u0026 ns[0] == '.' { continue } log.G(ctx).WithField(\"namespace\", ns).Debug(\"loading tasks in namespace\") if err := m.loadShims(namespaces.WithNamespace(ctx, ns)); err != nil { log.G(ctx).WithField(\"namespace\", ns).WithError(err).Error(\"loading tasks in namespace\") continue } if err := m.cleanupWorkDirs(namespaces.WithNamespace(ctx, ns)); err != nil { log.G(ctx).WithField(\"namespace\", ns).WithError(err).Error(\"cleanup working directory in namespace\") continue } } return nil } ns dir loadShims func (m *ShimManager) loadShims(ctx context.Context) error { ns, err := namespaces.NamespaceRequired(ctx) if err != nil { return err } shimDirs, err := os.ReadDir(filepath.Join(m.state, ns)) if err != nil { return err } for _, sd := range shimDirs { if !sd.IsDir() { continue } id := sd.Name() // skip hidden directories if len(id) \u003e 0 \u0026\u0026 id[0] == '.' { continue } bundle, err := LoadBundle(ctx, m.state, id) if err != nil { // fine to return error here, it is a programmer error if the context // does not have a namespace return err } // fast path bf, err := os.ReadDir(bundle.Path) if err != nil { bundle.Delete() log.G(ctx).WithError(err).Errorf(\"fast path read bundle path for %s\", bundle.Path) continue } if len(bf) == 0 { bundle.Delete() continue } var ( runtime string ) // If we're on 1.6+ and specified custom path to the runtime binary, path will be saved in 'shim-binary-path' file. if data, err := os.ReadFile(filepath.Join(bundle.Path, \"shim-binary-path\")); err == nil { runtime = string(data) } else if err != nil \u0026\u0026 !os.IsNotExist(err) { log.G(ctx).WithError(err).Error(\"failed to read `runtime` ","date":"2022-11-18","objectID":"/2022-11-18-containerd-9-containerd-containerd-shim-runc/:5:0","tags":["containerd","containerd-shim","runc"],"title":"Containerd解析(9) - Containerd,Containerd-shim,runc的依存关系","uri":"/2022-11-18-containerd-9-containerd-containerd-shim-runc/"},{"categories":null,"content":"第四种情况 containerd进程不存在的情况下，杀死容器进程，然后启动containerd进程 先杀掉 containerd进程，shim 和 容器进程都存在： [xiu-desktop] ~ sp shim [shim] 相关进程信息如下： root 140133 1 0 02:22 ? 00:00:00 /usr/bin/containerd-shim-runc-v2 -namespace default -id 7206459bb2db6c1636a9d2931e1b0edcd5ae82fd3426796d8959c76c3c81c1c8 -address /run/containerd/containerd.sock -debug [xiu-desktop] ~ sp main-example [main-example] 相关进程信息如下： root 140163 140133 0 02:22 ? 00:00:00 /main-example 然后杀掉容器进程，shim进程依然存在： [xiu-desktop] ~ sudo kill -9 140163 [xiu-desktop] ~ sp shim [shim] 相关进程信息如下： root 140133 1 0 02:22 ? 00:00:00 /usr/bin/containerd-shim-runc-v2 -namespace default -id 7206459bb2db6c1636a9d2931e1b0edcd5ae82fd3426796d8959c76c3c81c1c8 -address /run/containerd/containerd.sock -debug 然后重启containerd，shim进程消失 [xiu-desktop] ~ sp shim 没有 [shim] 相关进程 [xiu-desktop] ~ sp main-example 没有 [main-example] 相关进程 这时候容器变为 created 状态 [xiu-desktop] ~ n ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7206459bb2db docker.io/q946666800/runcdev:latest \"/main-example\" 4 minutes ago Created runcdev ","date":"2022-11-18","objectID":"/2022-11-18-containerd-9-containerd-containerd-shim-runc/:6:0","tags":["containerd","containerd-shim","runc"],"title":"Containerd解析(9) - Containerd,Containerd-shim,runc的依存关系","uri":"/2022-11-18-containerd-9-containerd-containerd-shim-runc/"},{"categories":null,"content":"概述 在runc中， 有两种 Terminal Modes，分别是：New Terminal、Pass-Through 有两种 Runc Modes，分别是：Foreground、Detached 接下来，通过runc中的代码，了解这几中 modes。 ","date":"2022-11-15","objectID":"/2022-11-15-containerd-8-runc-modes/:1:0","tags":["runc","log","io","modes","terminal"],"title":"Containerd解析(8) - Runc Modes","uri":"/2022-11-15-containerd-8-runc-modes/"},{"categories":null,"content":"setupIO // setupIO modifies the given process config according to the options. // Terminal Modes: // New Terminal // Pass-Through // runc Modes: // Foreground // Detached func setupIO(process *libcontainer.Process, rootuid, rootgid int, createTTY, detach bool, sockpath string) (*tty, error) { if createTTY { process.Stdin = nil process.Stdout = nil process.Stderr = nil t := \u0026tty{} if !detach { // 1, New Terminal \u0026 Foreground if err := t.initHostConsole(); err != nil { return nil, err } parent, child, err := utils.NewSockPair(\"console\") if err != nil { return nil, err } process.ConsoleSocket = child t.postStart = append(t.postStart, parent, child) t.consoleC = make(chan error, 1) go func() { t.consoleC \u003c- t.recvtty(parent) }() } else { // 2, New Terminal \u0026 Detached // the caller of runc will handle receiving the console master conn, err := net.Dial(\"unix\", sockpath) if err != nil { return nil, err } uc, ok := conn.(*net.UnixConn) if !ok { return nil, errors.New(\"casting to UnixConn failed\") } t.postStart = append(t.postStart, uc) socket, err := uc.File() if err != nil { return nil, err } t.postStart = append(t.postStart, socket) process.ConsoleSocket = socket } return t, nil } // when runc will detach the caller provides the stdio to runc via runc's 0,1,2 // and the container's process inherits runc's stdio. // 3, Pass-Through \u0026 Detached if detach { inheritStdio(process) return \u0026tty{}, nil } // 4, Pass-Through \u0026 Foreground return setupProcessPipes(process, rootuid, rootgid) } ","date":"2022-11-15","objectID":"/2022-11-15-containerd-8-runc-modes/:2:0","tags":["runc","log","io","modes","terminal"],"title":"Containerd解析(8) - Runc Modes","uri":"/2022-11-15-containerd-8-runc-modes/"},{"categories":null,"content":"New Terminal \u0026 Foreground process.Stdin = nil process.Stdout = nil process.Stderr = nil t := \u0026tty{} // 1, New Terminal \u0026 Foreground if err := t.initHostConsole(); err != nil { return nil, err } parent, child, err := utils.NewSockPair(\"console\") if err != nil { return nil, err } process.ConsoleSocket = child t.postStart = append(t.postStart, parent, child) t.consoleC = make(chan error, 1) go func() { t.consoleC \u003c- t.recvtty(parent) }() runc --root \\ /run/containerd/runc/default \\ run \\ -b \\ /home/xiu/CloudDrive/runcdev/mycontainer-terminal \\ demo Foreground模式下，runc进程会一直保留在前台，不会退出。 这里调用 t.recvtty(parent) func (t *tty) recvtty(socket *os.File) (Err error) { f, err := utils.RecvFd(socket) if err != nil { return err } cons, err := console.ConsoleFromFile(f) if err != nil { return err } err = console.ClearONLCR(cons.Fd()) if err != nil { return err } epoller, err := console.NewEpoller() if err != nil { return err } epollConsole, err := epoller.Add(cons) if err != nil { return err } defer func() { if Err != nil { _ = epollConsole.Close() } }() go func() { _ = epoller.Wait() }() go func() { _, _ = io.Copy(epollConsole, os.Stdin) }() t.wg.Add(1) go t.copyIO(os.Stdout, epollConsole) // Set raw mode for the controlling terminal. if err := t.hostConsole.SetRaw(); err != nil { return fmt.Errorf(\"failed to set the terminal from the stdin: %w\", err) } go handleInterrupt(t.hostConsole) t.epoller = epoller t.console = epollConsole t.closers = []io.Closer{epollConsole} return nil } 上面 init 之前， 下面是 init 过程， // 组装command 模版，生成init 命令，设置大量以 _xxx 这样格式的环境变量，即大量的extraFiles 文件 cmd := c.commandTemplate(p, childInitPipe, childLogPipe) func (c *Container) commandTemplate(p *Process, childInitPipe *os.File, childLogPipe *os.File) *exec.Cmd { cmd := exec.Command(\"/proc/self/exe\", \"init\") cmd.Args[0] = os.Args[0] // 当 New Terminal 模式时 // p.Stdin、p.Stdout、p.Stderr都被设置为nil cmd.Stdin = p.Stdin cmd.Stdout = p.Stdout cmd.Stderr = p.Stderr cmd.Dir = c.config.Rootfs if cmd.SysProcAttr == nil { cmd.SysProcAttr = \u0026unix.SysProcAttr{} } cmd.Env = append(cmd.Env, \"GOMAXPROCS=\"+os.Getenv(\"GOMAXPROCS\")) cmd.ExtraFiles = append(cmd.ExtraFiles, p.ExtraFiles...) if p.ConsoleSocket != nil { cmd.ExtraFiles = append(cmd.ExtraFiles, p.ConsoleSocket) cmd.Env = append(cmd.Env, \"_LIBCONTAINER_CONSOLE=\"+strconv.Itoa(stdioFdCount+len(cmd.ExtraFiles)-1), ) } cmd.ExtraFiles = append(cmd.ExtraFiles, childInitPipe) cmd.Env = append(cmd.Env, \"_LIBCONTAINER_INITPIPE=\"+strconv.Itoa(stdioFdCount+len(cmd.ExtraFiles)-1), \"_LIBCONTAINER_STATEDIR=\"+c.root, ) cmd.ExtraFiles = append(cmd.ExtraFiles, childLogPipe) cmd.Env = append(cmd.Env, \"_LIBCONTAINER_LOGPIPE=\"+strconv.Itoa(stdioFdCount+len(cmd.ExtraFiles)-1), \"_LIBCONTAINER_LOGLEVEL=\"+p.LogLevel, ) // NOTE: when running a container with no PID namespace and the parent process spawning the container is // PID1 the pdeathsig is being delivered to the container's init process by the kernel for some reason // even with the parent still running. if c.config.ParentDeathSignal \u003e 0 { cmd.SysProcAttr.Pdeathsig = unix.Signal(c.config.ParentDeathSignal) } return cmd } // runc/libcontainer/process_linux.go err := p.cmd.Start() init process 容器初始化过程中，查看父进程发送给子进程的初始化数据： // runc/libcontainer/process_linux.go func (p *initProcess) sendConfig() error { // send the config to the container's init process, we don't use JSON Encode // here because there might be a problem in JSON decoder in some cases, see: // https://github.com/docker/docker/issues/14203#issuecomment-174177790 return utils.WriteJSON(p.messageSockPair.parent, p.config) } p.config runc init 中： // 设置console if l.config.CreateConsole { if err := setupConsole(l.consoleSocket, l.config, true); err != nil { return err } if err := system.Setctty(); err != nil { return err } } // setupConsole sets up the console from inside the container, and sends the // master pty fd to the config.Pipe (using cmsg). This is done to ensure that // consoles are scoped to a container properly (see runc#814 and the many // i","date":"2022-11-15","objectID":"/2022-11-15-containerd-8-runc-modes/:2:1","tags":["runc","log","io","modes","terminal"],"title":"Containerd解析(8) - Runc Modes","uri":"/2022-11-15-containerd-8-runc-modes/"},{"categories":null,"content":"New Terminal \u0026 Detached // the caller of runc will handle receiving the console master conn, err := net.Dial(\"unix\", sockpath) if err != nil { return nil, err } uc, ok := conn.(*net.UnixConn) if !ok { return nil, errors.New(\"casting to UnixConn failed\") } t.postStart = append(t.postStart, uc) socket, err := uc.File() if err != nil { return nil, err } t.postStart = append(t.postStart, socket) process.ConsoleSocket = socket ","date":"2022-11-15","objectID":"/2022-11-15-containerd-8-runc-modes/:2:2","tags":["runc","log","io","modes","terminal"],"title":"Containerd解析(8) - Runc Modes","uri":"/2022-11-15-containerd-8-runc-modes/"},{"categories":null,"content":"Pass-Through \u0026 Detached if detach { inheritStdio(process) return \u0026tty{}, nil } func inheritStdio(process *libcontainer.Process) { process.Stdin = os.Stdin process.Stdout = os.Stdout process.Stderr = os.Stderr } ","date":"2022-11-15","objectID":"/2022-11-15-containerd-8-runc-modes/:2:3","tags":["runc","log","io","modes","terminal"],"title":"Containerd解析(8) - Runc Modes","uri":"/2022-11-15-containerd-8-runc-modes/"},{"categories":null,"content":"Pass-Through \u0026 Foreground 略 ","date":"2022-11-15","objectID":"/2022-11-15-containerd-8-runc-modes/:2:4","tags":["runc","log","io","modes","terminal"],"title":"Containerd解析(8) - Runc Modes","uri":"/2022-11-15-containerd-8-runc-modes/"},{"categories":null,"content":"概述 容器日志处理，其实就是处理runc程序的标准输出，标准错误。 这里我们分析 containerd-shim-runc-v2 与 runc 查看 shim 调用runc的代码 containerd/pkg/process/init.go // Create the process with the provided config func (p *Init) Create(ctx context.Context, r *CreateConfig) error { var ( err error socket *runc.Socket pio *processIO pidFile = newPidFile(p.Bundle) ) // 是否为 Terminal 模式，也就是 -t 模式 if r.Terminal { if socket, err = runc.NewTempConsoleSocket(); err != nil { return fmt.Errorf(\"failed to create OCI runtime console socket: %w\", err) } defer socket.Close() } else { if pio, err = createIO(ctx, p.id, p.IoUID, p.IoGID, p.stdio); err != nil { return fmt.Errorf(\"failed to create init process I/O: %w\", err) } // 除了 -t 模式，其他方式都在抽象成pio p.io = pio } if r.Checkpoint != \"\" { return p.createCheckpointedState(r, pidFile) } opts := \u0026runc.CreateOpts{ PidFile: pidFile.Path(), NoPivot: p.NoPivotRoot, NoNewKeyring: p.NoNewKeyring, } // pio赋值 if p.io != nil { opts.IO = p.io.IO() } // socket 赋值 if socket != nil { opts.ConsoleSocket = socket } // 前面把处理日志的方式抽象成 socket 或 pio // 在 p.runtime.Create 里面将 socket 或 pio 与 runc(容器进程)关联起来 if err := p.runtime.Create(ctx, r.ID, r.Bundle, opts); err != nil { return p.runtimeError(err, \"OCI runtime create failed\") } ... .. } // Create creates a new container and returns its pid if it was created successfully func (r *Runc) Create(context context.Context, id, bundle string, opts *CreateOpts) error { args := []string{\"create\", \"--bundle\", bundle} if opts != nil { oargs, err := opts.args() if err != nil { return err } args = append(args, oargs...) } cmd := r.command(context, append(args, id)...) if opts != nil \u0026\u0026 opts.IO != nil { opts.Set(cmd) } cmd.ExtraFiles = opts.ExtraFiles if cmd.Stdout == nil \u0026\u0026 cmd.Stderr == nil { data, err := cmdOutput(cmd, true, nil) defer putBuf(data) if err != nil { return fmt.Errorf(\"%s: %s\", err, data.String()) } return nil } ec, err := Monitor.Start(cmd) if err != nil { return err } if opts != nil \u0026\u0026 opts.IO != nil { if c, ok := opts.IO.(StartCloser); ok { if err := c.CloseAfterStart(); err != nil { return err } } } status, err := Monitor.Wait(cmd, ec) if err == nil \u0026\u0026 status != 0 { err = fmt.Errorf(\"%s did not terminate successfully: %w\", cmd.Args[0], \u0026ExitError{status}) } return err } socket 传递到runc的 --console-socket参数 这里查看下pio即几种处理方式 fifo // Set sets the io to the exec.Cmd func (i *pipeIO) Set(cmd *exec.Cmd) { if i.in != nil { cmd.Stdin = i.in.r } if i.out != nil { cmd.Stdout = i.out.w } if i.err != nil { cmd.Stderr = i.err.w } } binary func (b *binaryIO) Set(cmd *exec.Cmd) { if b.out != nil { cmd.Stdout = b.out.w } if b.err != nil { cmd.Stderr = b.err.w } } stdio func (s *stdio) Set(cmd *exec.Cmd) { cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr } ","date":"2022-11-14","objectID":"/2022-11-14-containerd-7-log/:1:0","tags":["containerd-shim","log","pipe"],"title":"Containerd解析(7) - log","uri":"/2022-11-14-containerd-7-log/"},{"categories":null,"content":"nerdctl 中的日志处理 分析两种常用的情况。 ","date":"2022-11-14","objectID":"/2022-11-14-containerd-7-log/:2:0","tags":["containerd-shim","log","pipe"],"title":"Containerd解析(7) - log","uri":"/2022-11-14-containerd-7-log/"},{"categories":null,"content":"nerdctl run -t –name runcdev1 q946666800/runcdev runc启动命令： runc启动命令 查看config.json配置 terminal: true 可知该模式为 New Terminal \u0026 Detached ，需要外部传入 --console-socket 从runc启动参数可以看到 12 = {string} \"--console-socket\" 13 = {string} \"/run/user/1000/pty4072274856/pty.sock\" 查看如何创建并监听 /run/user/1000/pty4072274856/pty.sock 查看代码，可以看到在shim的 runc.NewContainer() -\u003e p.Create() 代码中，创建了 pty.sock if r.Terminal { if socket, err = runc.NewTempConsoleSocket(); err != nil { return fmt.Errorf(\"failed to create OCI runtime console socket: %w\", err) } defer socket.Close() } else { if pio, err = createIO(ctx, p.id, p.IoUID, p.IoGID, p.stdio); err != nil { return fmt.Errorf(\"failed to create init process I/O: %w\", err) } p.io = pio } NewTempConsoleSocket()创建了 pty.sock，并将sock的路径参数传递给runc。 接下来查看 shim 创建的pty.sock怎样与nerdctl关联起来？？？ nerdctl： // var con console.Console if flagT { con = console.Current() defer con.Reset() if err := con.SetRaw(); err != nil { return err } } // nerdctl/pkg/taskutil/taskutil.go // 注意， cio.WithStreams(in, con, nil)这里传入了 in，con // con是nertctl进程本身的终端。 ioCreator = cio.NewCreator(cio.WithStreams(in, con, nil), cio.WithTerminal) // WithTerminal sets the terminal option func WithTerminal(opt *Streams) { opt.Terminal = true } // WithStreams sets the stream options to the specified Reader and Writers func WithStreams(stdin io.Reader, stdout, stderr io.Writer) Opt { return func(opt *Streams) { opt.Stdin = stdin opt.Stdout = stdout opt.Stderr = stderr } } // NewCreator returns an IO creator from the options func NewCreator(opts ...Opt) Creator { streams := \u0026Streams{} for _, opt := range opts { opt(streams) } if streams.FIFODir == \"\" { streams.FIFODir = defaults.DefaultFIFODir } // 所以，在这里的时候 return func(id string) (IO, error) { // 将 stdio 转换为字符串路径 fifos, err := NewFIFOSetInDir(streams.FIFODir, id, streams.Terminal) if err != nil { return nil, err } if streams.Stdin == nil { fifos.Stdin = \"\" } if streams.Stdout == nil { fifos.Stdout = \"\" } if streams.Stderr == nil { fifos.Stderr = \"\" } return copyIO(fifos, streams) } } 最终在 copyIO 中，会创建 /run/containerd/fifo/392418153 路径下创建相关sock，并与nerdctl终端相关联，如下： root@xiu-desktop:/run/containerd/fifo/7416553# tree . ├── 2e8e9f34cedc5bcca69af8e0e98afae4452463a3b5094b03b71a970f9d88eefa-stdin └── 2e8e9f34cedc5bcca69af8e0e98afae4452463a3b5094b03b71a970f9d88eefa-stdout 随后，nerdctl将这两个路径传递给containerd，containerd传递给shim。 shim： 在shim中，接收到的请求如下： // Create the process with the provided config func (p *Init) Create(ctx context.Context, r *CreateConfig) error { var ( err error socket *runc.Socket pio *processIO pidFile = newPidFile(p.Bundle) ) if r.Terminal { if socket, err = runc.NewTempConsoleSocket(); err != nil { return fmt.Errorf(\"failed to create OCI runtime console socket: %w\", err) } defer socket.Close() } else { ... } ... if socket != nil { opts.ConsoleSocket = socket } if err := p.runtime.Create(ctx, r.ID, r.Bundle, opts); err != nil { return p.runtimeError(err, \"OCI runtime create failed\") } ... ... if socket != nil { console, err := socket.ReceiveMaster() if err != nil { return fmt.Errorf(\"failed to retrieve console master: %w\", err) } console, err = p.Platform.CopyConsole(ctx, console, p.id, r.Stdin, r.Stdout, r.Stderr, \u0026p.wg) if err != nil { return fmt.Errorf(\"failed to start console copy: %w\", err) } p.console = console } ... ... return nil } func (p *linuxPlatform) CopyConsole(ctx context.Context, console console.Console, id, stdin, stdout, stderr string, wg *sync.WaitGroup) (cons console.Console, retErr error) { if p.epoller == nil { return nil, errors.New(\"uninitialized epoller\") } epollConsole, err := p.epoller.Add(console) if err != nil { return nil, err } var cwg sync.WaitGroup if stdin != \"\" { in, err := fifo.OpenFifo(context.Background(), stdin, syscall.O_RDONLY|syscall.O_NONBLOCK, 0) if err != nil { return nil, err } cwg.Add(1) go func() { cwg.Done() bp := bufPool.Get().(*[]byte) defer bufPool.Put(bp) io.CopyBuffer(epollConsole, in, *bp) // we need to shutdown epollConsole when pipe broken epollConsole.Shutdown","date":"2022-11-14","objectID":"/2022-11-14-containerd-7-log/:2:1","tags":["containerd-shim","log","pipe"],"title":"Containerd解析(7) - log","uri":"/2022-11-14-containerd-7-log/"},{"categories":null,"content":"nerdctl run -d –name runcdev1 q946666800/runcdev runc启动方式为：Pass-Through \u0026 Detached 首先，查看shim，create的请求参数： // Create a new initial process and container with the underlying OCI runtime func (s *service) Create(ctx context.Context, r *taskAPI.CreateTaskRequest) (_ *taskAPI.CreateTaskResponse, err error) { s.mu.Lock() defer s.mu.Unlock() container, err := runc.NewContainer(ctx, s.platform, r) if err != nil { return nil, err } s.containers[r.ID] = container s.send(\u0026eventstypes.TaskCreate{ ContainerID: r.ID, Bundle: r.Bundle, Rootfs: r.Rootfs, IO: \u0026eventstypes.TaskIO{ Stdin: r.Stdin, Stdout: r.Stdout, Stderr: r.Stderr, Terminal: r.Terminal, }, Checkpoint: r.Checkpoint, Pid: uint32(container.Pid()), }) return \u0026taskAPI.CreateTaskResponse{ Pid: uint32(container.Pid()), }, nil } 信息 注意，这里的 Stdio 与上面的区别。 在 -it 模式中，有Stdin, Stdout,没有Stderr，是因为我们使用终端模式运行，需要输入，并且Stdout和Stderr会统一输出到终端，所以合并在Stdout中 在-d模式中，因为进程是后台运行，所以不需要Stdin，Stdout和Stderr可以分开传输。 if r.Terminal { if socket, err = runc.NewTempConsoleSocket(); err != nil { return fmt.Errorf(\"failed to create OCI runtime console socket: %w\", err) } defer socket.Close() } else { if pio, err = createIO(ctx, p.id, p.IoUID, p.IoGID, p.stdio); err != nil { return fmt.Errorf(\"failed to create init process I/O: %w\", err) } p.io = pio } 这回执行的是 createIO func createIO(ctx context.Context, id string, ioUID, ioGID int, stdio stdio.Stdio) (*processIO, error) { pio := \u0026processIO{ stdio: stdio, } if stdio.IsNull() { i, err := runc.NewNullIO() if err != nil { return nil, err } pio.io = i return pio, nil } u, err := url.Parse(stdio.Stdout) if err != nil { return nil, fmt.Errorf(\"unable to parse stdout uri: %w\", err) } if u.Scheme == \"\" { u.Scheme = \"fifo\" } pio.uri = u // u.scheme = \"binary\" switch u.Scheme { case \"fifo\": pio.copy = true pio.io, err = runc.NewPipeIO(ioUID, ioGID, withConditionalIO(stdio)) case \"binary\": pio.io, err = NewBinaryIO(ctx, id, u) case \"file\": filePath := u.Path if err := os.MkdirAll(filepath.Dir(filePath), 0755); err != nil { return nil, err } var f *os.File f, err = os.OpenFile(filePath, os.O_APPEND|os.O_CREATE|os.O_WRONLY, 0644) if err != nil { return nil, err } f.Close() pio.stdio.Stdout = filePath pio.stdio.Stderr = filePath pio.copy = true pio.io, err = runc.NewPipeIO(ioUID, ioGID, withConditionalIO(stdio)) default: return nil, fmt.Errorf(\"unknown STDIO scheme %s\", u.Scheme) } if err != nil { return nil, err } return pio, nil } // NewBinaryIO runs a custom binary process for pluggable shim logging func NewBinaryIO(ctx context.Context, id string, uri *url.URL) (_ runc.IO, err error) { ns, err := namespaces.NamespaceRequired(ctx) if err != nil { return nil, err } var closers []func() error defer func() { if err == nil { return } result := multierror.Append(err) for _, fn := range closers { result = multierror.Append(result, fn()) } err = multierror.Flatten(result) }() out, err := newPipe() if err != nil { return nil, fmt.Errorf(\"failed to create stdout pipes: %w\", err) } closers = append(closers, out.Close) serr, err := newPipe() if err != nil { return nil, fmt.Errorf(\"failed to create stderr pipes: %w\", err) } closers = append(closers, serr.Close) r, w, err := os.Pipe() if err != nil { return nil, err } closers = append(closers, r.Close, w.Close) // 1, 配置日志驱动子进程 cmd := NewBinaryCmd(uri, id, ns) // 2, 配置子进程额外文件描述符 cmd.ExtraFiles = append(cmd.ExtraFiles, out.r, serr.r, w) // don't need to register this with the reaper or wait when // running inside a shim if err := cmd.Start(); err != nil { return nil, fmt.Errorf(\"failed to start binary process: %w\", err) } closers = append(closers, func() error { return cmd.Process.Kill() }) // close our side of the pipe after start if err := w.Close(); err != nil { return nil, fmt.Errorf(\"failed to close write pipe after start: %w\", err) } // wait for the logging binary to be ready b := make([]byte, 1) if _, err := r.Read(b); err != nil \u0026\u0026 err != io.EOF { return nil, fmt.Errorf(\"failed to read from logging binary: %w\", err) } return \u0026binaryIO{ cmd: ","date":"2022-11-14","objectID":"/2022-11-14-containerd-7-log/:2:2","tags":["containerd-shim","log","pipe"],"title":"Containerd解析(7) - log","uri":"/2022-11-14-containerd-7-log/"},{"categories":null,"content":"nerdctl-log-example 下面通过简单的代码，模拟 nerdctl run -d --name runcdev1 q946666800/runcdev日志的搜集过程。 // app.go package main import ( \"fmt\" \"time\" ) func main() { i := 5 for i \u003e 0 { fmt.Println(time.Now()) time.Sleep(time.Second) i-- } } // drive.go package main import ( \"bufio\" \"context\" \"encoding/json\" \"fmt\" \"io\" \"os\" \"os/signal\" \"sync\" \"time\" \"github.com/fahedouch/go-logrotate\" \"github.com/sirupsen/logrus\" \"golang.org/x/sys/unix\" ) func main() { fmt.Println(\"log drive start!!!\") ctx, cancel := context.WithCancel(context.Background()) defer cancel() var ( sigCh = make(chan os.Signal, 32) errCh = make(chan error, 1) ) signal.Notify(sigCh, unix.SIGTERM) // cmd.ExtraFiles = append(cmd.ExtraFiles, out.r, serr.r, w) out := os.NewFile(3, \"CONTAINER_STDOUT\") serr := os.NewFile(4, \"CONTAINER_STDERR\") wait := os.NewFile(5, \"CONTAINER_WAIT\") go func() { errCh \u003c- logger(ctx, out, serr, wait.Close) }() for { select { case \u003c-sigCh: cancel() case err := \u003c-errCh: if err != nil { fmt.Fprintln(os.Stderr, err) os.Exit(1) } fmt.Println(\"log drive exit 0\") os.Exit(0) } } } func logger(_ context.Context, out *os.File, serr *os.File, ready func() error) error { // Notify the shim that it is ready // call wait.Close // r will receive io.EOF error if err := ready(); err != nil { return err } // log path jsonFilePath := \"app.log\" l := \u0026logrotate.Logger{ Filename: jsonFilePath, } return Encode(l, out, serr) } // Entry is compatible with Docker \"json-file\" logs type Entry struct { Log string `json:\"log,omitempty\"` // line, including \"\\r\\n\" Stream string `json:\"stream,omitempty\"` // \"stdout\" or \"stderr\" Time time.Time `json:\"time\"` // e.g. \"2020-12-11T20:29:41.939902251Z\" } func Encode(w io.WriteCloser, stdout, stderr io.Reader) error { enc := json.NewEncoder(w) var encMu sync.Mutex var wg sync.WaitGroup wg.Add(2) f := func(r io.Reader, name string) { defer wg.Done() br := bufio.NewReader(r) e := \u0026Entry{ Stream: name, } for { line, err := br.ReadString(byte('\\n')) if err != nil { logrus.WithError(err).Errorf(\"failed to read line from %q\", name) return } e.Log = line e.Time = time.Now().UTC() encMu.Lock() encErr := enc.Encode(e) encMu.Unlock() if encErr != nil { logrus.WithError(err).Errorf(\"failed to encode JSON\") return } } } go f(stdout, \"stdout\") go f(stderr, \"stderr\") wg.Wait() return nil } // shim.go package main import ( \"fmt\" \"io\" \"log\" \"os\" \"os/exec\" ) func main() { // start log driver pio, err := driveIO() if err != nil { log.Fatal(err) } // start app cmd := exec.Command(\"./app-example\") cmd.Stdout = pio.out.w cmd.Stderr = pio.err.w err = cmd.Start() if err != nil { log.Fatal(err) } err = cmd.Wait() if err != nil { log.Fatal(err) } } func newPipe() (*pipe, error) { r, w, err := os.Pipe() if err != nil { return nil, err } return \u0026pipe{ r: r, w: w, }, nil } type pipe struct { r *os.File w *os.File } type binaryIO struct { cmd *exec.Cmd out, err *pipe } func (p *pipe) Close() error { if err := p.w.Close(); err != nil { } if err := p.r.Close(); err != nil { } return fmt.Errorf(\"pipe close error\") } func driveIO() (_ *binaryIO, err error) { var closers []func() error // app out pipe out, err := newPipe() if err != nil { return nil, err } closers = append(closers, out.Close) // app err pipe serr, err := newPipe() if err != nil { return nil, err } closers = append(closers, serr.Close) // drive ready pipe r, w, err := os.Pipe() if err != nil { return nil, err } closers = append(closers, r.Close, w.Close) cmd := exec.Command(\"./drive-example\") cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr cmd.ExtraFiles = append(cmd.ExtraFiles, out.r, serr.r, w) if err := cmd.Start(); err != nil { return nil, err } closers = append(closers, func() error { return cmd.Process.Kill() }) // close our side of the pipe after start if err := w.Close(); err != nil { return nil, fmt.Errorf(\"failed to close write pipe after start: %w\", err) } // wait for the logging binary to be ready b := make([]byte, 1) if _, err := r.Read(b); err != nil \u0026\u0026 err != io.EOF { return nil, fmt.Errorf(\"failed ","date":"2022-11-14","objectID":"/2022-11-14-containerd-7-log/:2:3","tags":["containerd-shim","log","pipe"],"title":"Containerd解析(7) - log","uri":"/2022-11-14-containerd-7-log/"},{"categories":null,"content":"kubelet中的日志处理 使用命令 k run nginx --image=nginx 创建pod，会启动两个容器，我们只关注业务容器，即nginx容器。 ","date":"2022-11-14","objectID":"/2022-11-14-containerd-7-log/:3:0","tags":["containerd-shim","log","pipe"],"title":"Containerd解析(7) - log","uri":"/2022-11-14-containerd-7-log/"},{"categories":null,"content":"概述 在runc的start命令代码如下： switch status { // 对于一个已经处于Created状态的容器，执行Exec case libcontainer.Created: return container.Exec() case libcontainer.Stopped: return errors.New(\"cannot start a container that has stopped\") case libcontainer.Running: return errors.New(\"cannot start an already running container\") default: return fmt.Errorf(\"cannot start a container in the %s state\\n\", status) } 说明已经 Stopped 的容器runc是不支持重新启动的。 但是在 docker、nerdctl 中，都存在 restart 命令。 接下来以 nerdctl 为例，看一下restart命令的实现。 if err := stopContainer(ctx, found.Container, timeout); err != nil { return err } if err := startContainer(ctx, found.Container, false, client); err != nil { return err } func startContainer(ctx context.Context, container containerd.Container, flagA bool, client *containerd.Client) error { ... ... if oldTask, err := container.Task(ctx, nil); err == nil { if _, err := oldTask.Delete(ctx); err != nil { logrus.WithError(err).Debug(\"failed to delete old task\") } } task, err := container.NewTask(ctx, taskCIO) if err != nil { return err } ... ... if err := task.Start(ctx); err != nil { return err } ... ... } 先oldTask.Delete，然后NewTask。 ","date":"2022-11-13","objectID":"/2022-11-13-containerd-6-restart/:1:0","tags":["containerd","restart"],"title":"Containerd解析(6) - restart","uri":"/2022-11-13-containerd-6-restart/"},{"categories":null,"content":"oldTask.Delete nerdctl -\u003e containerd -\u003e shim func (p *Init) delete(ctx context.Context) error { waitTimeout(ctx, \u0026p.wg, 2*time.Second) // 1 runc delete err := p.runtime.Delete(ctx, p.id, nil) // ignore errors if a runtime has already deleted the process // but we still hold metadata and pipes // // this is common during a checkpoint, runc will delete the container state // after a checkpoint and the container will no longer exist within runc if err != nil { if strings.Contains(err.Error(), \"does not exist\") { err = nil } else { err = p.runtimeError(err, \"failed to delete task\") } } if p.io != nil { for _, c := range p.closers { c.Close() } p.io.Close() } // 取消rootfs挂载 if err2 := mount.UnmountAll(p.Rootfs, 0); err2 != nil { log.G(ctx).WithError(err2).Warn(\"failed to cleanup rootfs mount\") if err == nil { err = fmt.Errorf(\"failed rootfs umount: %w\", err2) } } return err } 通过runc删除容器 err := p.runtime.Delete(ctx, p.id, nil) runc delete 取消rootfs挂载 if err2 := mount.UnmountAll(p.Rootfs, 0); err2 != nil { log.G(ctx).WithError(err2).Warn(\"failed to cleanup rootfs mount\") if err == nil { err = fmt.Errorf(\"failed rootfs umount: %w\", err2) } } 接下来通过 NewTask 重新创建容器，Start 启动容器，完成了容器的restart过程。 ","date":"2022-11-13","objectID":"/2022-11-13-containerd-6-restart/:2:0","tags":["containerd","restart"],"title":"Containerd解析(6) - restart","uri":"/2022-11-13-containerd-6-restart/"},{"categories":null,"content":"总结 从runc的角度来看，restart过程是删除了容器，再重新创建了一个同名容器。 ","date":"2022-11-13","objectID":"/2022-11-13-containerd-6-restart/:3:0","tags":["containerd","restart"],"title":"Containerd解析(6) - restart","uri":"/2022-11-13-containerd-6-restart/"},{"categories":null,"content":"概述 记录shim相关的疑问和分析 ","date":"2022-11-02","objectID":"/2022-11-02-containerd-3-shim-start/:1:0","tags":["containerd","containerd-shim"],"title":"Containerd解析(3) - shim","uri":"/2022-11-02-containerd-3-shim-start/"},{"categories":null,"content":"shim如何监控多个容器？ 在k8s中，创建一个pod，如：k run nginx --image=nginx 实际上会创建两个容器，一个是 pause 容器（也称为sandbox容器），一个是nginx容器。 这两个容器的父进程都是同一个shim，如下： ps -ef --forest 接下来通过代码调试，了解这个过程： 运行命令 k run nginx --image=nginx 创建pause容器 shim, err := m.startShim(ctx, bundle, id, opts) ... ... out, err := cmd.CombinedOutput() out的值 unix:///run/containerd/s/a27575c24e27006499fdd59b050358b4db870446a64a857fa02abae7d096a9b6 创建nginx容器 shim, err := m.startShim(ctx, bundle, id, opts) ... ... out, err := cmd.CombinedOutput() out的值 unix:///run/containerd/s/a27575c24e27006499fdd59b050358b4db870446a64a857fa02abae7d096a9b6 创建的容器如下： 可以看到，虽然调用了两次 shim, err := m.startShim(ctx, bundle, id, opts)，但是返回了相同的shim服务地址，查看新增的shim进程，也只有一个，所以： 信息 containerd为同一个pod创建容器时，只有pause容器启动了shim进程(shim服务端)，并返回一个shim的通信地址。 后续启动同一个pod的其他容器时，虽然也调用了m.startShim()，但不会启动新的shim进程，而且返回相同的通信地址。 所以，同一个pod下的容器，都是由同一个shim进程创建管理。 代码实现 查看shim start命令相关代码： func (manager) Start(ctx context.Context, id string, opts shim.StartOpts) (_ string, retErr error) { // 构建另一个shim cmd // 真正的 shim server是由这个cmd启动的 // 当前cmd返回了 sock address cmd, err := newCommand(ctx, id, opts.ContainerdBinary, opts.Address, opts.TTRPCAddress, opts.Debug) if err != nil { return \"\", err } // grouping 默认为 id grouping := id spec, err := readSpec() if err != nil { return \"\", err } // 查看 config.json 中是否存在 groupID for _, group := range groupLabels { if groupID, ok := spec.Annotations[group]; ok { grouping = groupID break } } // 计算shim服务端的address地址 // unix:///run/containerd/s/a27575c24e27006499fdd59b050358b4db870446a64a857fa02abae7d096a9b6 address, err := shim.SocketAddress(ctx, opts.Address, grouping) if err != nil { return \"\", err } // 尝试创建sock socket, err := shim.NewSocket(address) if err != nil { // the only time where this would happen is if there is a bug and the socket // was not cleaned up in the cleanup method of the shim or we are using the // grouping functionality where the new process should be run with the same // shim as an existing container if !shim.SocketEaddrinuse(err) { return \"\", fmt.Errorf(\"create new shim socket: %w\", err) } // 如果创建失败，可能已经创建过 // 尝试连接，并直接返回 address // pod中除了pause容器，其他容器属于这种情况 if shim.CanConnect(address) { if err := shim.WriteAddress(\"address\", address); err != nil { return \"\", fmt.Errorf(\"write existing socket for shim: %w\", err) } return address, nil } if err := shim.RemoveSocket(address); err != nil { return \"\", fmt.Errorf(\"remove pre-existing socket: %w\", err) } if socket, err = shim.NewSocket(address); err != nil { return \"\", fmt.Errorf(\"try create new shim socket 2x: %w\", err) } } defer func() { if retErr != nil { socket.Close() _ = shim.RemoveSocket(address) } }() // make sure that reexec shim-v2 binary use the value if need if err := shim.WriteAddress(\"address\", address); err != nil { return \"\", err } f, err := socket.File() if err != nil { return \"\", err } // 将文件描述符传递给子进程 cmd.ExtraFiles = append(cmd.ExtraFiles, f) goruntime.LockOSThread() if os.Getenv(\"SCHED_CORE\") != \"\" { if err := schedcore.Create(schedcore.ProcessGroup); err != nil { return \"\", fmt.Errorf(\"enable sched core support: %w\", err) } } if err := cmd.Start(); err != nil { f.Close() return \"\", err } goruntime.UnlockOSThread() defer func() { if retErr != nil { cmd.Process.Kill() } }() // make sure to wait after start go cmd.Wait() if data, err := io.ReadAll(os.Stdin); err == nil { if len(data) \u003e 0 { var any ptypes.Any if err := proto.Unmarshal(data, \u0026any); err != nil { return \"\", err } v, err := typeurl.UnmarshalAny(\u0026any) if err != nil { return \"\", err } if opts, ok := v.(*options.Options); ok { if opts.ShimCgroup != \"\" { if cgroups.Mode() == cgroups.Unified { cg, err := cgroupsv2.LoadManager(\"/sys/fs/cgroup\", opts.ShimCgroup) if err != nil { return \"\", fmt.Errorf(\"failed to load cgroup %s: %w\", opts.ShimCgroup, err) } if err := cg.AddProc(uint64(cmd.Process.Pid)); err != nil { return \"\", fmt.Errorf(\"failed to join cgroup %s: %w\", opts.ShimCgroup, err) } } else { cg, err := cgroups.Load(cgroups.V1, cgroups.StaticPath(opts.ShimCgroup)) if err !=","date":"2022-11-02","objectID":"/2022-11-02-containerd-3-shim-start/:2:0","tags":["containerd","containerd-shim"],"title":"Containerd解析(3) - shim","uri":"/2022-11-02-containerd-3-shim-start/"},{"categories":null,"content":"shim意外退出，会发生什么？ ","date":"2022-11-02","objectID":"/2022-11-02-containerd-3-shim-start/:3:0","tags":["containerd","containerd-shim"],"title":"Containerd解析(3) - shim","uri":"/2022-11-02-containerd-3-shim-start/"},{"categories":null,"content":"shim启动过程 主要分析 containerd-shim-runc-v2 package main import ( \"context\" \"github.com/containerd/containerd/runtime/v2/runc/manager\" _ \"github.com/containerd/containerd/runtime/v2/runc/pause\" _ \"github.com/containerd/containerd/runtime/v2/runc/task/plugin\" \"github.com/containerd/containerd/runtime/v2/shim\" ) func main() { shim.RunManager(context.Background(), manager.NewShimManager(\"io.containerd.runc.v2\")) } // NewShimManager returns an implementation of the shim manager // using runc func NewShimManager(name string) shim.Manager { return \u0026manager{ name: name, } } 这里使用的实现类为 // containerd/runtime/v2/runc/manager/manager_linux.go type manager struct { name string } // RunManager initialzes and runs a shim server // TODO(2.0): Rename to Run func RunManager(ctx context.Context, manager Manager, opts ...BinaryOpts) { var config Config for _, o := range opts { o(\u0026config) } ctx = log.WithLogger(ctx, log.G(ctx).WithField(\"runtime\", manager.Name())) if err := run(ctx, manager, nil, \"\", config); err != nil { fmt.Fprintf(os.Stderr, \"%s: %s\", manager.Name(), err) os.Exit(1) } } func run(ctx context.Context, manager Manager, initFunc Init, name string, config Config) error { parseFlags() if versionFlag { fmt.Printf(\"%s:\\n\", filepath.Base(os.Args[0])) fmt.Println(\" Version: \", version.Version) fmt.Println(\" Revision:\", version.Revision) fmt.Println(\" Go version:\", version.GoVersion) fmt.Println(\"\") return nil } if namespaceFlag == \"\" { return fmt.Errorf(\"shim namespace cannot be empty\") } setRuntime() // 1, 接收系统信号 // smp := []os.Signal{unix.SIGTERM, unix.SIGINT, unix.SIGPIPE} // smp = append(smp, unix.SIGCHLD) signals, err := setupSignals(config) if err != nil { return err } // prctl(PR_SET_CHILD_SUBREAPER,1) // 让当前进程像init进程一样来收养孤儿进程，称为subreaper进程 // 孤儿进程成会被祖先中距离最近的 supreaper 进程收养 if !config.NoSubreaper { if err := subreaper(); err != nil { return err } } ttrpcAddress := os.Getenv(ttrpcAddressEnv) publisher, err := NewPublisher(ttrpcAddress) if err != nil { return err } defer publisher.Close() ctx = namespaces.WithNamespace(ctx, namespaceFlag) ctx = context.WithValue(ctx, OptsKey{}, Opts{BundlePath: bundlePath, Debug: debugFlag}) ctx, sd := shutdown.WithShutdown(ctx) defer sd.Shutdown() // 初始化manager // 这里的manager并不为nil if manager == nil { service, err := initFunc(ctx, id, publisher, sd.Shutdown) if err != nil { return err } plugin.Register(\u0026plugin.Registration{ Type: plugin.TTRPCPlugin, ID: \"task\", Requires: []plugin.Type{ plugin.EventPlugin, }, InitFn: func(ic *plugin.InitContext) (interface{}, error) { return taskService{service}, nil }, }) manager = shimToManager{ shim: service, name: name, } } // Handle explicit actions switch action { case \"delete\": logger := log.G(ctx).WithFields(logrus.Fields{ \"pid\": os.Getpid(), \"namespace\": namespaceFlag, }) go reap(ctx, logger, signals) ss, err := manager.Stop(ctx, id) if err != nil { return err } data, err := proto.Marshal(\u0026shimapi.DeleteResponse{ Pid: uint32(ss.Pid), ExitStatus: uint32(ss.ExitStatus), ExitedAt: protobuf.ToTimestamp(ss.ExitedAt), }) if err != nil { return err } if _, err := os.Stdout.Write(data); err != nil { return err } return nil case \"start\": opts := StartOpts{ ContainerdBinary: containerdBinaryFlag, Address: addressFlag, TTRPCAddress: ttrpcAddress, Debug: debugFlag, } // 启动 shim server，并返回 sock address address, err := manager.Start(ctx, id, opts) if err != nil { return err } if _, err := os.Stdout.WriteString(address); err != nil { return err } return nil } if !config.NoSetupLogger { ctx, err = setLogger(ctx, id) if err != nil { return err } } plugin.Register(\u0026plugin.Registration{ Type: plugin.InternalPlugin, ID: \"shutdown\", InitFn: func(ic *plugin.InitContext) (interface{}, error) { return sd, nil }, }) // Register event plugin plugin.Register(\u0026plugin.Registration{ Type: plugin.EventPlugin, ID: \"publisher\", InitFn: func(ic *plugin.InitContext) (interface{}, error) { return publisher, nil }, }) var ( initialized = plugin.NewPluginSet() ttrpcServices = []ttrpcService{} ttrpcUna","date":"2022-11-02","objectID":"/2022-11-02-containerd-3-shim-start/:4:0","tags":["containerd","containerd-shim"],"title":"Containerd解析(3) - shim","uri":"/2022-11-02-containerd-3-shim-start/"},{"categories":null,"content":"shim Monitor shim对容器进程的监控，是shim进程比较核心的一个功能。接下来对该功能的实现进行解析。 首先查看启动过程中， TaskService 的启动代码： // NewTaskService creates a new instance of a task service func NewTaskService(ctx context.Context, publisher shim.Publisher, sd shutdown.Service) (taskAPI.TaskService, error) { var ( ep oom.Watcher err error ) if cgroups.Mode() == cgroups.Unified { ep, err = oomv2.New(publisher) } else { ep, err = oomv1.New(publisher) } if err != nil { return nil, err } go ep.Run(ctx) s := \u0026service{ context: ctx, events: make(chan interface{}, 128), // s.ec进行了订阅 // 这个订阅比较重要，发送通知的事件基本都是这个订阅产生的 ec: reaper.Default.Subscribe(), ep: ep, shutdown: sd, containers: make(map[string]*runc.Container), } // 从 s.ec 中获取退出事件，处理后发送到 s.events 中 go s.processExits() // 修改 runcC.Monitor 的实现 runcC.Monitor = reaper.Default if err := s.initPlatform(); err != nil { return nil, fmt.Errorf(\"failed to initialized platform behavior: %w\", err) } // 从 s.events 中获取时间，过滤处理之后，使用 publisher 发送事件 go s.forward(ctx, publisher) sd.RegisterCallback(func(context.Context) error { close(s.events) return nil }) if address, err := shim.ReadAddress(\"address\"); err == nil { sd.RegisterCallback(func(context.Context) error { return shim.RemoveSocket(address) }) } return s, nil } reap(ctx context.Context, logger *logrus.Entry, signals chan os.Signal) 主循环，持续监听 SIGCHLD 事件 func reap(ctx context.Context, logger *logrus.Entry, signals chan os.Signal) error { logger.Info(\"starting signal loop\") for { select { case \u003c-ctx.Done(): return ctx.Err() case s := \u003c-signals: // Exit signals are handled separately from this loop // They get registered with this channel so that we can ignore such signals for short-running actions (e.g. `delete`) switch s { case unix.SIGCHLD: / if err := reaper.Reap(); err != nil { logger.WithError(err).Error(\"reap exit status\") } case unix.SIGPIPE: } } } } Reap() error SIGCHLD 事件处理 // Reap should be called when the process receives an SIGCHLD. Reap will reap // all exited processes and close their wait channels func Reap() error { now := time.Now() // 1 exits, err := reap(false) for _, e := range exits { // 2 done := Default.notify(runc.Exit{ Timestamp: now, Pid: e.Pid, Status: e.Status, }) select { case \u003c-done: case \u003c-time.After(1 * time.Second): } } return err } reap(wait bool) (exits []exit, err error) 获取子进程退出状态 // reap reaps all child processes for the calling process and returns their // exit information func reap(wait bool) (exits []exit, err error) { var ( ws unix.WaitStatus rus unix.Rusage ) flag := unix.WNOHANG if wait { flag = 0 } for { pid, err := unix.Wait4(-1, \u0026ws, flag, \u0026rus) if err != nil { if err == unix.ECHILD { return exits, nil } return exits, err } if pid \u003c= 0 { return exits, nil } exits = append(exits, exit{ Pid: pid, Status: exitStatus(ws), }) } } notify(e runc.Exit) 将退出事件通知给各个订阅者。 这个函数看起来有些复杂，实际上可以理解为： func (m *Monitor) notify(e runc.Exit) chan struct{} { ... for _, s := range subscribers { s.c \u003c- e } ... } func (m *Monitor) notify(e runc.Exit) chan struct{} { const timeout = 1 * time.Millisecond var ( done = make(chan struct{}, 1) timer = time.NewTimer(timeout) success = make(map[chan runc.Exit]struct{}) ) stop(timer, true) go func() { defer close(done) for { var ( failed int subscribers = m.getSubscribers() ) for _, s := range subscribers { s.do(func() { if s.closed { return } if _, ok := success[s.c]; ok { return } timer.Reset(timeout) recv := true select { case s.c \u003c- e: success[s.c] = struct{}{} case \u003c-timer.C: recv = false failed++ } stop(timer, recv) }) } // all subscribers received the message if failed == 0 { return } } }() return done } Monitor.Start(cmd) \u0026 Monitor.Wait(cmd, ec) // Start starts the command a registers the process with the reaper func (m *Monitor) Start(c *exec.Cmd) (chan runc.Exit, error) { ec := m.Subscribe() if err := c.Start(); err != nil { m.Unsubscribe(ec) return nil, err } return ec, nil } // Wait blocks until a process is signal as dead. // User should rely on the value of the exit status to determine if the // command was successfu","date":"2022-11-02","objectID":"/2022-11-02-containerd-3-shim-start/:5:0","tags":["containerd","containerd-shim"],"title":"Containerd解析(3) - shim","uri":"/2022-11-02-containerd-3-shim-start/"},{"categories":null,"content":"概述 本文只讨论containerd默认配置下的行为，不涉及docker。 参考：content-flow.md 执行 client.Pull(ctx, \"docker.io/library/redis:5.0.9\", containerd.WithPullUnpack)之后， containerd获取的内容如下(已排序)： /var/lib/containerd/io.containerd.content.v1.content/blobs └── sha256 ├── 2a9865e55c37293b71df051922022898d8e4ec0f579c9b53a0caee1b170bc81c - the index ├── 9bb13890319dc01e5f8a4d3d0c4c72685654d682d568350fd38a02b1d70aee6b - the manifest for linux/amd64 ├── 987b553c835f01f46eb1859bc32f564119d5833801a27b25a0ca5c6b8b6e111a - the config ├── bb79b6b2107fea8e8a47133a660b78e3a546998fcf0427be39ac9a0af4a97e90 - layer 0 ├── 1ed3521a5dcbd05214eb7f35b952ecf018d5a6610c32ba4e315028c556f45e94 - layer 1 ├── 5999b99cee8f2875d391d64df20b6296b63f23951a7d41749f028375e887cd05 - layer 2 ├── bfee6cb5fdad6b60ec46297f44542ee9d8ac8f01c072313a51cd7822df3b576f - layer 3 ├── fd36a1ebc6728807cbb1aa7ef24a1861343c6dc174657721c496613c7b53bd07 - layer 4 └── 97481c7992ebf6f22636f87e4d7b79e962f928cdbe6f2337670fa6c9a9636f04 - layer 5 ➜ ~ ctr content ls（已整理排序） the index: sha256:2a9865e55c37293b71df051922022898d8e4ec0f579c9b53a0caee1b170bc81c containerd.io/gc.ref.content.m.4=sha256:ee0e1f8d8d338c9506b0e487ce6c2c41f931d1e130acd60dc7794c3a246eb59e, containerd.io/gc.ref.content.m.3=sha256:613f4797d2b6653634291a990f3e32378c7cfe3cdd439567b26ca340b8946013, containerd.io/gc.ref.content.m.0=sha256:9bb13890319dc01e5f8a4d3d0c4c72685654d682d568350fd38a02b1d70aee6b, containerd.io/gc.ref.content.m.1=sha256:aeb53f8db8c94d2cd63ca860d635af4307967aa11a2fdead98ae0ab3a329f470, containerd.io/gc.ref.content.m.6=sha256:4b7860fcaea5b9bbd6249c10a3dc02a5b9fb339e8aef17a542d6126a6af84d96, containerd.io/gc.ref.content.m.7=sha256:d66dfc869b619cd6da5b5ae9d7b1cbab44c134b31d458de07f7d580a84b63f69, containerd.io/gc.ref.content.m.5=sha256:1072145f8eea186dcedb6b377b9969d121a00e65ae6c20e9cd631483178ea7ed, containerd.io/gc.ref.content.m.2=sha256:17dc42e40d4af0a9e84c738313109f3a95e598081beef6c18a05abb57337aa5d, containerd.io/distribution.source.docker.io=library/redis, the manifest for linux/amd64 sha256:9bb13890319dc01e5f8a4d3d0c4c72685654d682d568350fd38a02b1d70aee6b containerd.io/gc.ref.content.config=sha256:987b553c835f01f46eb1859bc32f564119d5833801a27b25a0ca5c6b8b6e111a, containerd.io/gc.ref.content.l.0=sha256:bb79b6b2107fea8e8a47133a660b78e3a546998fcf0427be39ac9a0af4a97e90, containerd.io/gc.ref.content.l.1=sha256:1ed3521a5dcbd05214eb7f35b952ecf018d5a6610c32ba4e315028c556f45e94, containerd.io/gc.ref.content.l.2=sha256:5999b99cee8f2875d391d64df20b6296b63f23951a7d41749f028375e887cd05, containerd.io/gc.ref.content.l.3=sha256:bfee6cb5fdad6b60ec46297f44542ee9d8ac8f01c072313a51cd7822df3b576f, containerd.io/gc.ref.content.l.4=sha256:fd36a1ebc6728807cbb1aa7ef24a1861343c6dc174657721c496613c7b53bd07, containerd.io/gc.ref.content.l.5=sha256:97481c7992ebf6f22636f87e4d7b79e962f928cdbe6f2337670fa6c9a9636f04, containerd.io/distribution.source.docker.io=library/redis the config sha256:987b553c835f01f46eb1859bc32f564119d5833801a27b25a0ca5c6b8b6e111a containerd.io/gc.ref.snapshot.overlayfs=sha256:33bd296ab7f37bdacff0cb4a5eb671bcb3a141887553ec4157b1e64d6641c1cd, containerd.io/distribution.source.docker.io=library/redis sha256:bb79b6b2107fea8e8a47133a660b78e3a546998fcf0427be39ac9a0af4a97e90 containerd.io/uncompressed=sha256:d0fe97fa8b8cefdffcef1d62b65aba51a6c87b6679628a2b50fc6a7a579f764c sha256:1ed3521a5dcbd05214eb7f35b952ecf018d5a6610c32ba4e315028c556f45e94 containerd.io/uncompressed=sha256:832f21763c8e6b070314e619ebb9ba62f815580da6d0eaec8a1b080bd01575f7 sha256:5999b99cee8f2875d391d64df20b6296b63f23951a7d41749f028375e887cd05 containerd.io/uncompressed=sha256:223b15010c47044b6bab9611c7a322e8da7660a8268949e18edde9c6e3ea3700 sha256:bfee6cb5fdad6b60ec46297f44542ee9d8ac8f01c072313a51cd7822df3b576f containerd.io/uncompressed=sha256:b96fedf8ee00e59bf69cf5bc8ed19e92e66ee8cf83f0174e33127402b650331d sha256:fd36a1ebc6728807cbb1aa7ef24a1861343c6dc174657721c496613c7b53bd07 containerd.io/uncompressed=sha256:aff00695be0cebb8a114f8c5187fd6dd3d806273004797a00ad934ec9cd98212 ","date":"2022-11-02","objectID":"/2022-11-02-containerd-2-layer/:1:0","tags":["containerd","layer","finallayer"],"title":"Containerd解析(2) - layer","uri":"/2022-11-02-containerd-2-layer/"},{"categories":null,"content":"通过 diff_ids 找到 Final Layer Final Layer 也可以通过 config 配置文件中的 diff_ids 数组求出来 \"diff_ids\": [ \"sha256:d0fe97fa8b8cefdffcef1d62b65aba51a6c87b6679628a2b50fc6a7a579f764c\", \"sha256:832f21763c8e6b070314e619ebb9ba62f815580da6d0eaec8a1b080bd01575f7\", \"sha256:223b15010c47044b6bab9611c7a322e8da7660a8268949e18edde9c6e3ea3700\", \"sha256:b96fedf8ee00e59bf69cf5bc8ed19e92e66ee8cf83f0174e33127402b650331d\", \"sha256:aff00695be0cebb8a114f8c5187fd6dd3d806273004797a00ad934ec9cd98212\", \"sha256:d442ae63d423b4b1922875c14c3fa4e801c66c689b69bfd853758fde996feffb\" ] func TestFinalLayer(t *testing.T) { diffIDs := []digest.Digest{ \"sha256:d0fe97fa8b8cefdffcef1d62b65aba51a6c87b6679628a2b50fc6a7a579f764c\", \"sha256:832f21763c8e6b070314e619ebb9ba62f815580da6d0eaec8a1b080bd01575f7\", \"sha256:223b15010c47044b6bab9611c7a322e8da7660a8268949e18edde9c6e3ea3700\", \"sha256:b96fedf8ee00e59bf69cf5bc8ed19e92e66ee8cf83f0174e33127402b650331d\", \"sha256:aff00695be0cebb8a114f8c5187fd6dd3d806273004797a00ad934ec9cd98212\", \"sha256:d442ae63d423b4b1922875c14c3fa4e801c66c689b69bfd853758fde996feffb\"} finalLayer := identity.ChainID(diffIDs).String() fmt.Println(finalLayer) } === RUN TestFinalLayer sha256:33bd296ab7f37bdacff0cb4a5eb671bcb3a141887553ec4157b1e64d6641c1cd --- PASS: TestFinalLayer (0.00s) PASS 创建容器的时候，需要可读写层。这个可读写层的parent就是这里提到的Final Layer ","date":"2022-11-02","objectID":"/2022-11-02-containerd-2-layer/:2:0","tags":["containerd","layer","finallayer"],"title":"Containerd解析(2) - layer","uri":"/2022-11-02-containerd-2-layer/"},{"categories":null,"content":"diff_ids // TODO config 配置文件中的 diff_ids 数组是如何生成的？ ","date":"2022-11-02","objectID":"/2022-11-02-containerd-2-layer/:3:0","tags":["containerd","layer","finallayer"],"title":"Containerd解析(2) - layer","uri":"/2022-11-02-containerd-2-layer/"},{"categories":null,"content":"概述 本文使用containerd默认配置，不涉及docker。 通过一个简单的例子，粗略了解containerd创建并运行一个容器的过程。 主要了解containerd怎样与containerd-shim-runc-v2进行交互，以及containerd-shim-runc-v2怎样调用runc与监控容器。 代码版本： 1c90a442489720eec95342e1789ee8a5e1b9536f ","date":"2022-10-30","objectID":"/2022-10-30-containerd-1/:1:0","tags":["containerd","containerd-shim"],"title":"Containerd解析(1) - example","uri":"/2022-10-30-containerd-1/"},{"categories":null,"content":"Example 使用官方的例子，对containerd，containerd-shim-runc-v2 的代码进行debug package main import ( \"context\" \"fmt\" \"log\" \"syscall\" \"time\" \"github.com/containerd/containerd\" \"github.com/containerd/containerd/cio\" \"github.com/containerd/containerd/oci\" \"github.com/containerd/containerd/namespaces\" ) func main() { if err := redisExample(); err != nil { log.Fatal(err) } } func redisExample() error { // create a new client connected to the default socket path for containerd client, err := containerd.New(\"/run/containerd/containerd.sock\") if err != nil { return err } defer client.Close() // create a new context with an \"example\" namespace // 这里将 example 改为 ctx := namespaces.WithNamespace(context.Background(), \"default\") // pull the redis image from DockerHub image, err := client.Pull(ctx, \"docker.io/library/redis:alpine\", containerd.WithPullUnpack) if err != nil { return err } // create a container container, err := client.NewContainer( ctx, \"redis-server\", containerd.WithImage(image), containerd.WithNewSnapshot(\"redis-server-snapshot\", image), containerd.WithNewSpec(oci.WithImageConfig(image)), ) if err != nil { return err } defer container.Delete(ctx, containerd.WithSnapshotCleanup) // create a task from the container task, err := container.NewTask(ctx, cio.NewCreator(cio.WithStdio)) if err != nil { return err } defer task.Delete(ctx) // make sure we wait before calling start exitStatusC, err := task.Wait(ctx) if err != nil { fmt.Println(err) } // call start on the task to execute the redis server if err := task.Start(ctx); err != nil { return err } // sleep for a lil bit to see the logs time.Sleep(3 * time.Second) // kill the process and get the exit status if err := task.Kill(ctx, syscall.SIGTERM); err != nil { return err } // wait for the process to fully exit and print out the exit status status := \u003c-exitStatusC code, _, err := status.Result() if err != nil { return err } fmt.Printf(\"redis-server exited with status: %d\\n\", code) return nil } ","date":"2022-10-30","objectID":"/2022-10-30-containerd-1/:2:0","tags":["containerd","containerd-shim"],"title":"Containerd解析(1) - example","uri":"/2022-10-30-containerd-1/"},{"categories":null,"content":"[client] client.Pull() // pull the redis image from DockerHub image, err := client.Pull(ctx,\"docker.io/library/redis:alpine\",containerd.WithPullUnpack) if err != nil { return err } containerd启动时，初始文件夹如下 /var/lib/containerd/ ├── io.containerd.content.v1.content │ └── ingest ├── io.containerd.metadata.v1.bolt │ └── meta.db ├── io.containerd.runtime.v1.linux ├── io.containerd.runtime.v2.task ├── io.containerd.snapshotter.v1.aufs │ └── snapshots ├── io.containerd.snapshotter.v1.btrfs ├── io.containerd.snapshotter.v1.native │ └── snapshots ├── io.containerd.snapshotter.v1.overlayfs │ └── snapshots └── tmpmounts /run/containerd/ ├── containerd.sock ├── containerd.sock.ttrpc ├── io.containerd.runtime.v1.linux └── io.containerd.runtime.v2.tas // Pull downloads the provided content into containerd's content store // and returns a platform specific image object func (c *Client) Pull(ctx context.Context, ref string, opts ...RemoteOpt) (_ Image, retErr error) { pullCtx := defaultRemoteContext() for _, o := range opts { if err := o(c, pullCtx); err != nil { return nil, err } } if pullCtx.PlatformMatcher == nil { if len(pullCtx.Platforms) \u003e 1 { return nil, errors.New(\"cannot pull multiplatform image locally, try Fetch\") } else if len(pullCtx.Platforms) == 0 { pullCtx.PlatformMatcher = c.platform // MatchComparer 能够匹配和比较平台以过滤和排序平台。 } else { p, err := platforms.Parse(pullCtx.Platforms[0]) if err != nil { return nil, fmt.Errorf(\"invalid platform %s: %w\", pullCtx.Platforms[0], err) } pullCtx.PlatformMatcher = platforms.Only(p) } } ctx, done, err := c.WithLease(ctx) if err != nil { return nil, err } defer done(ctx) var unpacks int32 var unpackEg *errgroup.Group var unpackWrapper func(f images.Handler) images.Handler if pullCtx.Unpack { // unpacker only supports schema 2 image, for schema 1 this is noop. u, err := c.newUnpacker(ctx, pullCtx) if err != nil { return nil, fmt.Errorf(\"create unpacker: %w\", err) } unpackWrapper, unpackEg = u.handlerWrapper(ctx, pullCtx, \u0026unpacks) defer func() { if err := unpackEg.Wait(); err != nil { if retErr == nil { retErr = fmt.Errorf(\"unpack: %w\", err) } } }() wrapper := pullCtx.HandlerWrapper pullCtx.HandlerWrapper = func(h images.Handler) images.Handler { if wrapper == nil { return unpackWrapper(h) } return unpackWrapper(wrapper(h)) } } // 获取镜像的主要逻辑都在 fetch 方法 img, err := c.fetch(ctx, pullCtx, ref, 1) if err != nil { return nil, err } // NOTE(fuweid): unpacker defers blobs download. before create image // record in ImageService, should wait for unpacking(including blobs // download). if pullCtx.Unpack { if unpackEg != nil { // 等待镜像相关文件下载完成 if err := unpackEg.Wait(); err != nil { return nil, err } } } // 调用containerd接口，往 /var/lib/containerd/io.containerd.metadata.v1.bolt/meta.db // 数据库写入一个 images img, err = c.createNewImage(ctx, img) if err != nil { return nil, err } i := NewImageWithPlatform(c, img, pullCtx.PlatformMatcher) if pullCtx.Unpack { if unpacks == 0 { // Try to unpack is none is done previously. // This is at least required for schema 1 image. if err := i.Unpack(ctx, pullCtx.Snapshotter, pullCtx.UnpackOpts...); err != nil { return nil, fmt.Errorf(\"failed to unpack image on snapshotter %s: %w\", pullCtx.Snapshotter, err) } } } return i, nil } 查看fetch方法： func (c *Client) fetch(ctx context.Context, rCtx *RemoteContext, ref string, limit int) (images.Image, error) { store := c.ContentStore() // 通过 https://registry-1.docker.io/v2/library/redis/manifests/5.0.9 获取 Digest // 内容大致如下：sha256:2a9865e55c37293b71df051922022898d8e4ec0f579c9b53a0caee1b170bc81c name, desc, err := rCtx.Resolver.Resolve(ctx, ref) if err != nil { return images.Image{}, fmt.Errorf(\"failed to resolve reference %q: %w\", ref, err) } fetcher, err := rCtx.Resolver.Fetcher(ctx, name) if err != nil { return images.Image{}, fmt.Errorf(\"failed to get fetcher for %q: %w\", name, err) } var ( handler images.Handler isConvertible bool converterFunc func(context.Context, ocispec.Descriptor) (ocispec.Descriptor, error) limiter *semaphore.Weighted ) if de","date":"2022-10-30","objectID":"/2022-10-30-containerd-1/:3:0","tags":["containerd","containerd-shim"],"title":"Containerd解析(1) - example","uri":"/2022-10-30-containerd-1/"},{"categories":null,"content":"[client] client.NewContainer() // create a container container, err := client.NewContainer( ctx, \"redis-server\", containerd.WithImage(image), containerd.WithNewSnapshot(\"redis-server-snapshot\", image), containerd.WithNewSpec(oci.WithImageConfig(image)), ) NewContainer()方法如下： // NewContainer will create a new container with the provided id. // The id must be unique within the namespace. func (c *Client) NewContainer(ctx context.Context, id string, opts ...NewContainerOpts) (Container, error) { ctx, done, err := c.WithLease(ctx) if err != nil { return nil, err } defer done(ctx) container := containers.Container{ ID: id, Runtime: containers.RuntimeInfo{ Name: c.runtime, }, } for _, o := range opts { if err := o(ctx, c, \u0026container); err != nil { return nil, err } } r, err := c.ContainerService().Create(ctx, container) if err != nil { return nil, err } return containerFromRecord(c, r), nil } 查看前面传入的参数： \"redis-server\", containerd.WithImage(image), containerd.WithNewSnapshot(\"redis-server-snapshot\", image), containerd.WithNewSpec(oci.WithImageConfig(image)), iredis-server为容器id，其他三个为初始化选项。 ","date":"2022-10-30","objectID":"/2022-10-30-containerd-1/:4:0","tags":["containerd","containerd-shim"],"title":"Containerd解析(1) - example","uri":"/2022-10-30-containerd-1/"},{"categories":null,"content":"WithImage： // WithImage sets the provided image as the base for the container // 将提供的图像设置为容器的基础 func WithImage(i Image) NewContainerOpts { return func(ctx context.Context, client *Client, c *containers.Container) error { c.Image = i.Name() return nil } } ","date":"2022-10-30","objectID":"/2022-10-30-containerd-1/:4:1","tags":["containerd","containerd-shim"],"title":"Containerd解析(1) - example","uri":"/2022-10-30-containerd-1/"},{"categories":null,"content":"WithNewSpec： // WithNewSpec generates a new spec for a new container // 为新容器生成新规范 func WithNewSpec(opts ...oci.SpecOpts) NewContainerOpts { return func(ctx context.Context, client *Client, c *containers.Container) error { s, err := oci.GenerateSpec(ctx, client, c, opts...) if err != nil { return err } c.Spec, err = typeurl.MarshalAny(s) return err } } ","date":"2022-10-30","objectID":"/2022-10-30-containerd-1/:4:2","tags":["containerd","containerd-shim"],"title":"Containerd解析(1) - example","uri":"/2022-10-30-containerd-1/"},{"categories":null,"content":"WithNewSnapshot: // WithNewSnapshot allocates a new snapshot to be used by the container as the // root filesystem in read-write mode // 分配一个新的快照供容器用作读写模式下的根文件系统 func WithNewSnapshot(id string, i Image, opts ...snapshots.Opt) NewContainerOpts { return func(ctx context.Context, client *Client, c *containers.Container) error { // 从 meta.db 中拿到config，从而拿到diffIDs diffIDs, err := i.RootFS(ctx) if err != nil { return err } // 根据 diffIDs 获取 ChainIDs // 进而获取 parent（即镜像的finalLayer，作为可读写层的parent） parent := identity.ChainID(diffIDs).String() c.Snapshotter, err = client.resolveSnapshotterName(ctx, c.Snapshotter) if err != nil { return err } s, err := client.getSnapshotter(ctx, c.Snapshotter) if err != nil { return err } // Prepare if _, err := s.Prepare(ctx, id, parent, opts...); err != nil { return err } c.SnapshotKey = id c.Image = i.Name() return nil } } func (p *proxySnapshotter) Prepare(ctx context.Context, key, parent string, opts ...snapshots.Opt) ([]mount.Mount, error) { var local snapshots.Info for _, opt := range opts { if err := opt(\u0026local); err != nil { return nil, err } } // 调用 containerd 的 snapshots.Prepare()接口 resp, err := p.client.Prepare(ctx, \u0026snapshotsapi.PrepareSnapshotRequest{ Snapshotter: p.snapshotterName, Key: key, Parent: parent, Labels: local.Labels, }) if err != nil { return nil, errdefs.FromGRPC(err) } return toMounts(resp.Mounts), nil } [containerd] (s *service) Prepare /containerd/services/snapshots/service.go L88 func (s *service) Prepare(ctx context.Context, pr *snapshotsapi.PrepareSnapshotRequest) (*snapshotsapi.PrepareSnapshotResponse, error) { log.G(ctx).WithField(\"parent\", pr.Parent).WithField(\"key\", pr.Key).Debugf(\"prepare snapshot\") sn, err := s.getSnapshotter(pr.Snapshotter) if err != nil { return nil, err } var opts []snapshots.Opt if pr.Labels != nil { opts = append(opts, snapshot.WithLabels(pr.Labels)) } // 创建 snapshot mounts, err := sn.Prepare(ctx, pr.Key, pr.Parent, opts...) if err != nil { return nil, errdefs.ToGRPC(err) } return \u0026snapshotsapi.PrepareSnapshotResponse{ Mounts: fromMounts(mounts), }, nil } func (s *snapshotter) Prepare(ctx context.Context, key, parent string, opts ...snapshots.Opt) ([]mount.Mount, error) { // 创建可读写层并入库 mounts, err := s.Snapshotter.Prepare(ctx, key, parent, opts...) if err != nil { return nil, err } // 发送event？这部分内容待补充 if err := s.publisher.Publish(ctx, \"/snapshot/prepare\", \u0026eventstypes.SnapshotPrepare{ Key: key, Parent: parent, Snapshotter: s.name, }); err != nil { return nil, err } return mounts, nil } Prepare方法执行完后，新增了一个Snapshot，如下： ➜ ~ ctr snapshot ls [sudo] password for xiu: KEY PARENT KIND redis-server-snapshot sha256:33bd296ab7f37bdacff0cb4a5eb671bcb3a141887553ec4157b1e64d6641c1cd Active sha256:2ae5fa95c0fce5ef33fbb87a7e2f49f2a56064566a37a83b97d3f668c10b43d6 sha256:d0fe97fa8b8cefdffcef1d62b65aba51a6c87b6679628a2b50fc6a7a579f764c Committed sha256:33bd296ab7f37bdacff0cb4a5eb671bcb3a141887553ec4157b1e64d6641c1cd sha256:bc8b010e53c5f20023bd549d082c74ef8bfc237dc9bbccea2e0552e52bc5fcb1 Committed sha256:a8f09c4919857128b1466cc26381de0f9d39a94171534f63859a662d50c396ca sha256:2ae5fa95c0fce5ef33fbb87a7e2f49f2a56064566a37a83b97d3f668c10b43d6 Committed sha256:aa4b58e6ece416031ce00869c5bf4b11da800a397e250de47ae398aea2782294 sha256:a8f09c4919857128b1466cc26381de0f9d39a94171534f63859a662d50c396ca Committed sha256:bc8b010e53c5f20023bd549d082c74ef8bfc237dc9bbccea2e0552e52bc5fcb1 sha256:aa4b58e6ece416031ce00869c5bf4b11da800a397e250de47ae398aea2782294 Committed sha256:d0fe97fa8b8cefdffcef1d62b65aba51a6c87b6679628a2b50fc6a7a579f764c Committed 可以看到该snapshot的key是我们传进的参数redis-server-snapshot，parent是刚才提到的镜像的final layer：33bd296ab7f37bdacff0cb4a5eb671bcb3a141887553ec4157b1e64d6641c1cd。 ","date":"2022-10-30","objectID":"/2022-10-30-containerd-1/:4:3","tags":["containerd","containerd-shim"],"title":"Containerd解析(1) - example","uri":"/2022-10-30-containerd-1/"},{"categories":null,"content":"[containerd] c.ContainerService().Create() 接下来进入create流程，直接调用containerd的create接口。 containerd/services/containers/local.go L116 func (l *local) Create(ctx context.Context, req *api.CreateContainerRequest, _ ...grpc.CallOption) (*api.CreateContainerResponse, error) { var resp api.CreateContainerResponse if err := l.withStoreUpdate(ctx, func(ctx context.Context) error { container := containerFromProto(req.Container) created, err := l.Store.Create(ctx, container) if err != nil { return err } resp.Container = containerToProto(\u0026created) return nil }); err != nil { return \u0026resp, errdefs.ToGRPC(err) } // 发送event？这部分内容待补充 if err := l.publisher.Publish(ctx, \"/containers/create\", \u0026eventstypes.ContainerCreate{ ID: resp.Container.ID, Image: resp.Container.Image, Runtime: \u0026eventstypes.ContainerCreate_Runtime{ Name: resp.Container.Runtime.Name, Options: resp.Container.Runtime.Options, }, }); err != nil { return \u0026resp, err } return \u0026resp, nil } container参数内容如下： 入库的内容如下： 使用 nerdctl查看，如下： ➜ ~ sudo nerdctl ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES redis-server docker.io/library/redis:5.0.9 \"docker-entrypoint.s…\" 8 minutes ago Created 由上面分析可知，create操作只在数据库中插入了一条数据。 ","date":"2022-10-30","objectID":"/2022-10-30-containerd-1/:4:4","tags":["containerd","containerd-shim"],"title":"Containerd解析(1) - example","uri":"/2022-10-30-containerd-1/"},{"categories":null,"content":"[client] container.NewTask() func (c *container) NewTask(ctx context.Context, ioCreate cio.Creator, opts ...NewTaskOpts) (_ Task, err error) { i, err := ioCreate(c.id) if err != nil { return nil, err } defer func() { if err != nil \u0026\u0026 i != nil { i.Cancel() i.Close() } }() // 配置三个std // Stdin=/run/containerd/fifo/908671294/redis-server-stdin // Stdout=/run/containerd/fifo/908671294/redis-server-stdout // Stderr=/run/containerd/fifo/908671294/redis-server-stderr cfg := i.Config() request := \u0026tasks.CreateTaskRequest{ ContainerID: c.id, Terminal: cfg.Terminal, Stdin: cfg.Stdin, Stdout: cfg.Stdout, Stderr: cfg.Stderr, } r, err := c.get(ctx) if err != nil { return nil, err } if r.SnapshotKey != \"\" { if r.Snapshotter == \"\" { return nil, fmt.Errorf(\"unable to resolve rootfs mounts without snapshotter on container: %w\", errdefs.ErrInvalidArgument) } // get the rootfs from the snapshotter and add it to the request s, err := c.client.getSnapshotter(ctx, r.Snapshotter) if err != nil { return nil, err } // Options[0]: index=off // Options[1]: workdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/7/work // Options[2]: upperdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/7/fs // Options[3]: lowerdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/6/fs:/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/5/fs:/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/4/fs:/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/3/fs:/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/2/fs:/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/1/fs mounts, err := s.Mounts(ctx, r.SnapshotKey) if err != nil { return nil, err } spec, err := c.Spec(ctx) if err != nil { return nil, err } for _, m := range mounts { if spec.Linux != nil \u0026\u0026 spec.Linux.MountLabel != \"\" { context := label.FormatMountLabel(\"\", spec.Linux.MountLabel) if context != \"\" { m.Options = append(m.Options, context) } } request.Rootfs = append(request.Rootfs, \u0026types.Mount{ Type: m.Type, Source: m.Source, Options: m.Options, }) } } info := TaskInfo{ runtime: r.Runtime.Name, } for _, o := range opts { if err := o(ctx, c.client, \u0026info); err != nil { return nil, err } } if info.RootFS != nil { for _, m := range info.RootFS { request.Rootfs = append(request.Rootfs, \u0026types.Mount{ Type: m.Type, Source: m.Source, Options: m.Options, }) } } if info.Options != nil { any, err := typeurl.MarshalAny(info.Options) if err != nil { return nil, err } request.Options = any } t := \u0026task{ client: c.client, io: i, id: c.id, c: c, } if info.Checkpoint != nil { request.Checkpoint = info.Checkpoint } // response, err := c.client.TaskService().Create(ctx, request) if err != nil { return nil, errdefs.FromGRPC(err) } t.pid = response.Pid return t, nil } 最终组成的request请求参数如下： task request 调用containerd TaskService().Create()接口。 ","date":"2022-10-30","objectID":"/2022-10-30-containerd-1/:5:0","tags":["containerd","containerd-shim"],"title":"Containerd解析(1) - example","uri":"/2022-10-30-containerd-1/"},{"categories":null,"content":"[containerd] (l *local) Create() containerd/services/tasks/local.go L164 func (l *local) Create(ctx context.Context, r *api.CreateTaskRequest, _ ...grpc.CallOption) (*api.CreateTaskResponse, error) { container, err := l.getContainer(ctx, r.ContainerID) if err != nil { return nil, errdefs.ToGRPC(err) } checkpointPath, err := getRestorePath(container.Runtime.Name, r.Options) if err != nil { return nil, err } // jump get checkpointPath from checkpoint image if checkpointPath == \"\" \u0026\u0026 r.Checkpoint != nil { checkpointPath, err = os.MkdirTemp(os.Getenv(\"XDG_RUNTIME_DIR\"), \"ctrd-checkpoint\") if err != nil { return nil, err } if r.Checkpoint.MediaType != images.MediaTypeContainerd1Checkpoint { return nil, fmt.Errorf(\"unsupported checkpoint type %q\", r.Checkpoint.MediaType) } reader, err := l.store.ReaderAt(ctx, ocispec.Descriptor{ MediaType: r.Checkpoint.MediaType, Digest: digest.Digest(r.Checkpoint.Digest), Size: r.Checkpoint.Size, Annotations: r.Checkpoint.Annotations, }) if err != nil { return nil, err } _, err = archive.Apply(ctx, checkpointPath, content.NewReader(reader)) reader.Close() if err != nil { return nil, err } } opts := runtime.CreateOpts{ Spec: container.Spec, IO: runtime.IO{ Stdin: r.Stdin, Stdout: r.Stdout, Stderr: r.Stderr, Terminal: r.Terminal, }, Checkpoint: checkpointPath, Runtime: container.Runtime.Name, RuntimeOptions: container.Runtime.Options, TaskOptions: r.Options, SandboxID: container.SandboxID, } if r.RuntimePath != \"\" { opts.Runtime = r.RuntimePath } for _, m := range r.Rootfs { opts.Rootfs = append(opts.Rootfs, mount.Mount{ Type: m.Type, Source: m.Source, Options: m.Options, }) } if strings.HasPrefix(container.Runtime.Name, \"io.containerd.runtime.v1.\") { log.G(ctx).Warn(\"runtime v1 is deprecated since containerd v1.4, consider using runtime v2\") } else if container.Runtime.Name == plugin.RuntimeRuncV1 { log.G(ctx).Warnf(\"%q is deprecated since containerd v1.4, consider using %q\", plugin.RuntimeRuncV1, plugin.RuntimeRuncV2) } rtime, err := l.getRuntime(container.Runtime.Name) if err != nil { return nil, err } _, err = rtime.Get(ctx, r.ContainerID) if err != nil \u0026\u0026 !errdefs.IsNotFound(err) { return nil, errdefs.ToGRPC(err) } if err == nil { return nil, errdefs.ToGRPC(fmt.Errorf(\"task %s: %w\", r.ContainerID, errdefs.ErrAlreadyExists)) } // Create 启动新的 shim 实例并创建新任务 c, err := rtime.Create(ctx, r.ContainerID, opts) if err != nil { return nil, errdefs.ToGRPC(err) } labels := map[string]string{\"runtime\": container.Runtime.Name} if err := l.monitor.Monitor(c, labels); err != nil { return nil, fmt.Errorf(\"monitor task: %w\", err) } pid, err := c.PID(ctx) if err != nil { return nil, fmt.Errorf(\"failed to get task pid: %w\", err) } return \u0026api.CreateTaskResponse{ ContainerID: r.ContainerID, Pid: pid, }, nil } rtime.Create() rtime.Create(ctx, r.ContainerID, opts)是个比较重要的方法，终于看到和 shim 相关的内容了 // Create launches new shim instance and creates new task func (m *TaskManager) Create(ctx context.Context, taskID string, opts runtime.CreateOpts) (runtime.Task, error) { // 1 shim, err := m.manager.Start(ctx, taskID, opts) if err != nil { return nil, fmt.Errorf(\"failed to start shim: %w\", err) } // Cast to shim task and call task service to create a new container task instance. // This will not be required once shim service / client implemented. shimTask := newShimTask(shim) // 2 t, err := shimTask.Create(ctx, opts) if err != nil { // NOTE: ctx contains required namespace information. m.manager.shims.Delete(ctx, taskID) dctx, cancel := timeout.WithContext(context.Background(), cleanupTimeout) defer cancel() sandboxed := opts.SandboxID != \"\" _, errShim := shimTask.delete(dctx, sandboxed, func(context.Context, string) {}) if errShim != nil { if errdefs.IsDeadlineExceeded(errShim) { dctx, cancel = timeout.WithContext(context.Background(), cleanupTimeout) defer cancel() } shimTask.Shutdown(dctx) shimTask.Client().Close() } return nil, fmt.Errorf(\"failed to create shim task: %w\", err) } return t, nil } m.manager.Start() // Start launc","date":"2022-10-30","objectID":"/2022-10-30-containerd-1/:5:1","tags":["containerd","containerd-shim"],"title":"Containerd解析(1) - example","uri":"/2022-10-30-containerd-1/"},{"categories":null,"content":"[containerd-shim-runc-v2] Create() containerd/runtime/v2/runc/task/service.go L118 // Create a new initial process and container with the underlying OCI runtime func (s *service) Create(ctx context.Context, r *taskAPI.CreateTaskRequest) (_ *taskAPI.CreateTaskResponse, err error) { s.mu.Lock() defer s.mu.Unlock() container, err := runc.NewContainer(ctx, s.platform, r) if err != nil { return nil, err } s.containers[r.ID] = container s.send(\u0026eventstypes.TaskCreate{ ContainerID: r.ID, Bundle: r.Bundle, Rootfs: r.Rootfs, IO: \u0026eventstypes.TaskIO{ Stdin: r.Stdin, Stdout: r.Stdout, Stderr: r.Stderr, Terminal: r.Terminal, }, Checkpoint: r.Checkpoint, Pid: uint32(container.Pid()), }) return \u0026taskAPI.CreateTaskResponse{ Pid: uint32(container.Pid()), }, nil } runc.NewContainer(ctx, s.platform, r) // NewContainer returns a new runc container func NewContainer(ctx context.Context, platform stdio.Platform, r *task.CreateTaskRequest) (_ *Container, retErr error) { ns, err := namespaces.NamespaceRequired(ctx) if err != nil { return nil, fmt.Errorf(\"create namespace: %w\", err) } opts := \u0026options.Options{} if r.Options.GetValue() != nil { v, err := typeurl.UnmarshalAny(r.Options) if err != nil { return nil, err } if v != nil { opts = v.(*options.Options) } } var mounts []process.Mount for _, m := range r.Rootfs { mounts = append(mounts, process.Mount{ Type: m.Type, Source: m.Source, Target: m.Target, Options: m.Options, }) } rootfs := \"\" if len(mounts) \u003e 0 { rootfs = filepath.Join(r.Bundle, \"rootfs\") if err := os.Mkdir(rootfs, 0711); err != nil \u0026\u0026 !os.IsExist(err) { return nil, err } } config := \u0026process.CreateConfig{ ID: r.ID, Bundle: r.Bundle, Runtime: opts.BinaryName, Rootfs: mounts, Terminal: r.Terminal, Stdin: r.Stdin, Stdout: r.Stdout, Stderr: r.Stderr, Checkpoint: r.Checkpoint, ParentCheckpoint: r.ParentCheckpoint, Options: r.Options, } if err := WriteOptions(r.Bundle, opts); err != nil { return nil, err } // For historical reason, we write opts.BinaryName as well as the entire opts if err := WriteRuntime(r.Bundle, opts.BinaryName); err != nil { return nil, err } defer func() { if retErr != nil { if err := mount.UnmountAll(rootfs, 0); err != nil { logrus.WithError(err).Warn(\"failed to cleanup rootfs mount\") } } }() // 在这里进行rootfs的挂载 for _, rm := range mounts { m := \u0026mount.Mount{ Type: rm.Type, Source: rm.Source, Options: rm.Options, } if err := m.Mount(rootfs); err != nil { return nil, fmt.Errorf(\"failed to mount rootfs component %v: %w\", m, err) } } p, err := newInit( ctx, r.Bundle, filepath.Join(r.Bundle, \"work\"), ns, platform, config, opts, rootfs, ) if err != nil { return nil, errdefs.ToGRPC(err) } // 组装 p 和 config，准备调用runc if err := p.Create(ctx, config); err != nil { return nil, errdefs.ToGRPC(err) } container := \u0026Container{ ID: r.ID, Bundle: r.Bundle, process: p, processes: make(map[string]process.Process), reservedProcess: make(map[string]struct{}), } pid := p.Pid() if pid \u003e 0 { var cg interface{} if cgroups.Mode() == cgroups.Unified { g, err := cgroupsv2.PidGroupPath(pid) if err != nil { logrus.WithError(err).Errorf(\"loading cgroup2 for %d\", pid) return container, nil } cg, err = cgroupsv2.LoadManager(\"/sys/fs/cgroup\", g) if err != nil { logrus.WithError(err).Errorf(\"loading cgroup2 for %d\", pid) } } else { cg, err = cgroups.Load(cgroups.V1, cgroups.PidPath(pid)) if err != nil { logrus.WithError(err).Errorf(\"loading cgroup for %d\", pid) } } container.cgroup = cg } return container, nil } 在 p.Create(ctx, config)之前，有一个比较重要的操作，就是 m.Mount(rootfs) m.Mount(rootfs) for _, rm := range mounts { m := \u0026mount.Mount{ Type: rm.Type, Source: rm.Source, Options: rm.Options, } if err := m.Mount(rootfs); err != nil { return nil, fmt.Errorf(\"failed to mount rootfs component %v: %w\", m, err) } } m.Mount(rootfs)执行了挂载。 相关参数如下： m 其中，Options[3]的内容如下： lowerdir= /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/6/fs: /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/5/fs: /var/lib/containerd/io.containerd.snapshotter.","date":"2022-10-30","objectID":"/2022-10-30-containerd-1/:5:2","tags":["containerd","containerd-shim"],"title":"Containerd解析(1) - example","uri":"/2022-10-30-containerd-1/"},{"categories":null,"content":"[containerd] l.monitor.Monitor(c, labels) func (m *cgroupsMonitor) Monitor(c runtime.Task, labels map[string]string) error { if err := m.collector.Add(c, labels); err != nil { return err } t, ok := c.(*linux.Task) if !ok { return nil } cg, err := t.Cgroup() if err != nil { if errdefs.IsNotFound(err) { return nil } return err } err = m.oom.Add(c.ID(), c.Namespace(), cg, m.trigger) if err == cgroups.ErrMemoryNotSupported { logrus.WithError(err).Warn(\"OOM monitoring failed\") return nil } return err } ","date":"2022-10-30","objectID":"/2022-10-30-containerd-1/:5:3","tags":["containerd","containerd-shim"],"title":"Containerd解析(1) - example","uri":"/2022-10-30-containerd-1/"},{"categories":null,"content":"小结 从上文的分析可以看出，NewTask()完成了容器启动前的所有准备工作。 containerd创建task相关文件夹(也就是runc所需要的bundle)，并创建spec(也就是config.json)。 shim根据配置挂载rootfs，然后调用runc -b ... create创建容器，等待runc start启动容器。 ","date":"2022-10-30","objectID":"/2022-10-30-containerd-1/:5:4","tags":["containerd","containerd-shim"],"title":"Containerd解析(1) - example","uri":"/2022-10-30-containerd-1/"},{"categories":null,"content":"[client] task.Start(ctx) // call start on the task to execute the redis server if err := task.Start(ctx); err != nil { return err } ","date":"2022-10-30","objectID":"/2022-10-30-containerd-1/:6:0","tags":["containerd","containerd-shim"],"title":"Containerd解析(1) - example","uri":"/2022-10-30-containerd-1/"},{"categories":null,"content":"[containerd] (l *local) Start() func (l *local) Start(ctx context.Context, r *api.StartRequest, _ ...grpc.CallOption) (*api.StartResponse, error) { t, err := l.getTask(ctx, r.ContainerID) if err != nil { return nil, err } p := runtime.Process(t) if r.ExecID != \"\" { if p, err = t.Process(ctx, r.ExecID); err != nil { return nil, errdefs.ToGRPC(err) } } // containerd-shim-runc-v2 if err := p.Start(ctx); err != nil { return nil, errdefs.ToGRPC(err) } state, err := p.State(ctx) if err != nil { return nil, errdefs.ToGRPC(err) } return \u0026api.StartResponse{ Pid: state.Pid, }, nil } [containerd-shim-runc-v2] p.Start(ctx) // Start a process func (s *service) Start(ctx context.Context, r *taskAPI.StartRequest) (*taskAPI.StartResponse, error) { container, err := s.getContainer(r.ID) if err != nil { return nil, err } // hold the send lock so that the start events are sent before any exit events in the error case s.eventSendMu.Lock() // p, err := container.Start(ctx, r) if err != nil { s.eventSendMu.Unlock() return nil, errdefs.ToGRPC(err) } switch r.ExecID { case \"\": switch cg := container.Cgroup().(type) { case cgroups.Cgroup: if err := s.ep.Add(container.ID, cg); err != nil { logrus.WithError(err).Error(\"add cg to OOM monitor\") } case *cgroupsv2.Manager: allControllers, err := cg.RootControllers() if err != nil { logrus.WithError(err).Error(\"failed to get root controllers\") } else { if err := cg.ToggleControllers(allControllers, cgroupsv2.Enable); err != nil { if userns.RunningInUserNS() { logrus.WithError(err).Debugf(\"failed to enable controllers (%v)\", allControllers) } else { logrus.WithError(err).Errorf(\"failed to enable controllers (%v)\", allControllers) } } } if err := s.ep.Add(container.ID, cg); err != nil { logrus.WithError(err).Error(\"add cg to OOM monitor\") } } s.send(\u0026eventstypes.TaskStart{ ContainerID: container.ID, Pid: uint32(p.Pid()), }) default: s.send(\u0026eventstypes.TaskExecStarted{ ContainerID: container.ID, ExecID: r.ExecID, Pid: uint32(p.Pid()), }) } s.eventSendMu.Unlock() return \u0026taskAPI.StartResponse{ Pid: uint32(p.Pid()), }, nil } // Start a container process func (c *Container) Start(ctx context.Context, r *task.StartRequest) (process.Process, error) { p, err := c.Process(r.ExecID) if err != nil { return nil, err } // if err := p.Start(ctx); err != nil { return nil, err } if c.Cgroup() == nil \u0026\u0026 p.Pid() \u003e 0 { var cg interface{} if cgroups.Mode() == cgroups.Unified { g, err := cgroupsv2.PidGroupPath(p.Pid()) if err != nil { logrus.WithError(err).Errorf(\"loading cgroup2 for %d\", p.Pid()) } cg, err = cgroupsv2.LoadManager(\"/sys/fs/cgroup\", g) if err != nil { logrus.WithError(err).Errorf(\"loading cgroup2 for %d\", p.Pid()) } } else { cg, err = cgroups.Load(cgroups.V1, cgroups.PidPath(p.Pid())) if err != nil { logrus.WithError(err).Errorf(\"loading cgroup for %d\", p.Pid()) } } c.cgroup = cg } return p, nil } // Start the init process func (p *Init) Start(ctx context.Context) error { p.mu.Lock() defer p.mu.Unlock() // return p.initState.Start(ctx) } func (s *createdState) Start(ctx context.Context) error { // if err := s.p.start(ctx); err != nil { return err } return s.transition(\"running\") } func (p *Init) start(ctx context.Context) error { // err := p.runtime.Start(ctx, p.id) return p.runtimeError(err, \"OCI runtime start failed\") } // Start will start an already created container func (r *Runc) Start(context context.Context, id string) error { // return r.runOrError(r.command(context, \"start\", id)) } // runOrError will run the provided command. If an error is // encountered and neither Stdout or Stderr was set the error and the // stderr of the command will be returned in the format of \u003cerror\u003e: // \u003cstderr\u003e func (r *Runc) runOrError(cmd *exec.Cmd) error { if cmd.Stdout != nil || cmd.Stderr != nil { ec, err := Monitor.Start(cmd) if err != nil { return err } status, err := Monitor.Wait(cmd, ec) if err == nil \u0026\u0026 status != 0 { err = fmt.Errorf(\"%s did not terminate successfully: %w\", cmd.Args[0], \u0026ExitError{status}) } return er","date":"2022-10-30","objectID":"/2022-10-30-containerd-1/:6:1","tags":["containerd","containerd-shim"],"title":"Containerd解析(1) - example","uri":"/2022-10-30-containerd-1/"},{"categories":null,"content":"[client] task.Kill() // kill the process and get the exit status if err := task.Kill(ctx, syscall.SIGTERM); err != nil { return err } ","date":"2022-10-30","objectID":"/2022-10-30-containerd-1/:7:0","tags":["containerd","containerd-shim"],"title":"Containerd解析(1) - example","uri":"/2022-10-30-containerd-1/"},{"categories":null,"content":"[containerd] (l *local) Kill() func (l *local) Kill(ctx context.Context, r *api.KillRequest, _ ...grpc.CallOption) (*ptypes.Empty, error) { t, err := l.getTask(ctx, r.ContainerID) if err != nil { return nil, err } p := runtime.Process(t) if r.ExecID != \"\" { if p, err = t.Process(ctx, r.ExecID); err != nil { return nil, errdefs.ToGRPC(err) } } // if err := p.Kill(ctx, r.Signal, r.All); err != nil { return nil, errdefs.ToGRPC(err) } return empty, nil } func (s *shimTask) Kill(ctx context.Context, signal uint32, all bool) error { // if _, err := s.task.Kill(ctx, \u0026task.KillRequest{ ID: s.ID(), Signal: signal, All: all, }); err != nil { return errdefs.FromGRPC(err) } return nil } [containerd-shim-runc-v2] s.task.Kill() // Kill a process with the provided signal func (s *service) Kill(ctx context.Context, r *taskAPI.KillRequest) (*ptypes.Empty, error) { container, err := s.getContainer(r.ID) if err != nil { return nil, err } // if err := container.Kill(ctx, r); err != nil { return nil, errdefs.ToGRPC(err) } return empty, nil } // Kill a process func (c *Container) Kill(ctx context.Context, r *task.KillRequest) error { p, err := c.Process(r.ExecID) if err != nil { return err } // return p.Kill(ctx, r.Signal, r.All) } // Kill the init process func (p *Init) Kill(ctx context.Context, signal uint32, all bool) error { p.mu.Lock() defer p.mu.Unlock() // return p.initState.Kill(ctx, signal, all) } func (s *runningState) Kill(ctx context.Context, sig uint32, all bool) error { // return s.p.kill(ctx, sig, all) } func (p *Init) kill(ctx context.Context, signal uint32, all bool) error { // err := p.runtime.Kill(ctx, p.id, int(signal), \u0026runc.KillOpts{ All: all, }) return checkKillError(err) } // Kill sends the specified signal to the container func (r *Runc) Kill(context context.Context, id string, sig int, opts *KillOpts) error { args := []string{ \"kill\", } if opts != nil { args = append(args, opts.args()...) } // return r.runOrError(r.command(context, append(args, id, strconv.Itoa(sig))...)) } // runOrError will run the provided command. If an error is // encountered and neither Stdout or Stderr was set the error and the // stderr of the command will be returned in the format of \u003cerror\u003e: // \u003cstderr\u003e func (r *Runc) runOrError(cmd *exec.Cmd) error { if cmd.Stdout != nil || cmd.Stderr != nil { ec, err := Monitor.Start(cmd) if err != nil { return err } status, err := Monitor.Wait(cmd, ec) if err == nil \u0026\u0026 status != 0 { err = fmt.Errorf(\"%s did not terminate successfully: %w\", cmd.Args[0], \u0026ExitError{status}) } return err } // data, err := cmdOutput(cmd, true, nil) defer putBuf(data) if err != nil { return fmt.Errorf(\"%s: %s\", err, data.String()) } return nil } // callers of cmdOutput are expected to call putBuf on the returned Buffer // to ensure it is released back to the shared pool after use. func cmdOutput(cmd *exec.Cmd, combined bool, started chan\u003c- int) (*bytes.Buffer, error) { b := getBuf() cmd.Stdout = b if combined { cmd.Stderr = b } // ec, err := Monitor.Start(cmd) if err != nil { return nil, err } if started != nil { started \u003c- cmd.Process.Pid } status, err := Monitor.Wait(cmd, ec) if err == nil \u0026\u0026 status != 0 { err = fmt.Errorf(\"%s did not terminate successfully: %w\", cmd.Args[0], \u0026ExitError{status}) } return b, err } // Start starts the command a registers the process with the reaper func (m *Monitor) Start(c *exec.Cmd) (chan runc.Exit, error) { ec := m.Subscribe() if err := c.Start(); err != nil { m.Unsubscribe(ec) return nil, err } return ec, nil } 同样的，在 containerd-shim-runc-v2中套了很多层调用，最终还是回到了 (m *Monitor) Start() 查看此时的cmd参数： 发送了 runc kill redis-server 15 命令。 执行完成后，容器状态如下： ➜ ~ n ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES redis-server docker.io/library/redis:5.0.9 \"docker-entrypoint.s…\" 50 minutes ago Exited (0) About a minute ago ➜ ~ ➜ ~ ➜ ~ runc list ID PID STATUS BUNDLE CREATED OWNER redis-server 0 stopped /run/containerd/io.containerd.runtime.v2.task/default/redis-server 2022-11-03T03:04:04.47687884Z root ➜ ~ ➜ ","date":"2022-10-30","objectID":"/2022-10-30-containerd-1/:7:1","tags":["containerd","containerd-shim"],"title":"Containerd解析(1) - example","uri":"/2022-10-30-containerd-1/"},{"categories":null,"content":"defer task.Delete() ","date":"2022-10-30","objectID":"/2022-10-30-containerd-1/:8:0","tags":["containerd","containerd-shim"],"title":"Containerd解析(1) - example","uri":"/2022-10-30-containerd-1/"},{"categories":null,"content":"defer container.Delete() ","date":"2022-10-30","objectID":"/2022-10-30-containerd-1/:9:0","tags":["containerd","containerd-shim"],"title":"Containerd解析(1) - example","uri":"/2022-10-30-containerd-1/"},{"categories":null,"content":"总结 本文通过一个简单的例子，忽略了较多细节，了解一个容器在containerd中的主要启动过程。 在containerd中，有container和task两个概念， container是containerd中的概念，创建一个 container 的时候，containerd需要准备相关的镜像和目录 task是真正运行的部分，由具体的运行时创建的容器进程。 container是分配和附加资源的元数据对象，task是系统中动态的运行的的进程，task在每次运行后应该被删除掉，但是container能够被多次使用，更新，查询。 ","date":"2022-10-30","objectID":"/2022-10-30-containerd-1/:10:0","tags":["containerd","containerd-shim"],"title":"Containerd解析(1) - example","uri":"/2022-10-30-containerd-1/"},{"categories":null,"content":"1 配置 参考：https://hugoloveit.com/zh-cn/posts/ ","date":"2022-10-29","objectID":"/2022-10-29-my-first-post/:1:0","tags":["test"],"title":"My First Post","uri":"/2022-10-29-my-first-post/"},{"categories":null,"content":"2 图片 ","date":"2022-10-29","objectID":"/2022-10-29-my-first-post/:2:0","tags":["test"],"title":"My First Post","uri":"/2022-10-29-my-first-post/"},{"categories":null,"content":"上传配置 typora配置： typora setting 图片插入： # 使用typora直接拖拽即可 ![image-20221030223715217](https://raw.githubusercontent.com/yzxiu/images/blog/2022-10/20221030-223715.png \"typora setting\") picgo-core插件： # https://connor-sun.github.io/posts/38835.html picgo install super-prefix picgo-core配置： { \"picBed\": { \"github\": { \"repo\": \"*****\", \"token\": \"*****\", \"path\": \"/\", \"customUrl\": \"\", \"branch\": \"blog\" }, \"current\": \"github\", \"uploader\": \"github\" }, \"picgoPlugins\": { \"picgo-plugin-super-prefix\": true }, \"picgo-plugin-super-prefix\": { \"prefixFormat\": \"YYYY-MM/\", \"fileFormat\": \"YYYYMMDD-HHmmss\" } } ","date":"2022-10-29","objectID":"/2022-10-29-my-first-post/:2:1","tags":["test"],"title":"My First Post","uri":"/2022-10-29-my-first-post/"},{"categories":null,"content":"小图展示 小图片如果使用 ![image-20221030223715217](https://raw.githubusercontent.com/yzxiu/images/blog/2022-10/20221030-235308.png \"small image\")的方式插入，会拉伸宽屏，如下： small image 可以改用 \u003cimg\u003e 的方式 ，并使用 {{\u003c style \"text-align:center;\" \u003e}} 控制位置，如下： {{\u003c style \"text-align:center;\" \u003e}} \u003cimg src=\"https://raw.githubusercontent.com/yzxiu/images/blog/2022-10/20221030-235308.png\" style=\"zoom: 80%;\" \u003e {{\u003c /style \u003e}} 呈现的效果如下： 图文混排，使用style=\"zoom: 20%;\"控制大小，如下： 图文混排\u003cimg src=\"https://raw.githubusercontent.com/yzxiu/images/blog/2022-10/20221030-235308.png\" style=\"zoom: 10%;\" /\u003e图文混排 图文混排\u003cimg src=\"https://raw.githubusercontent.com/yzxiu/images/blog/2022-10/20221030-235308.png\" style=\"zoom: 20%;\" /\u003e图文混排 图文混排\u003cimg src=\"https://raw.githubusercontent.com/yzxiu/images/blog/2022-10/20221030-235308.png\" style=\"zoom: 30%;\" /\u003e图文混排 呈现的效果如下： 图文混排图文混排 图文混排图文混排 图文混排图文混排 ","date":"2022-10-29","objectID":"/2022-10-29-my-first-post/:2:2","tags":["test"],"title":"My First Post","uri":"/2022-10-29-my-first-post/"},{"categories":null,"content":"3 代码 package main import \"fmt\" func split(sum int) (x, y int) { x = sum * 4 / 9 y = sum - x return } func main() { fmt.Println(split(17)) } ","date":"2022-10-29","objectID":"/2022-10-29-my-first-post/:3:0","tags":["test"],"title":"My First Post","uri":"/2022-10-29-my-first-post/"},{"categories":null,"content":"4 横幅 {{\u003c admonition \u003e}} 一个 **注意** 横幅 {{\u003c /admonition \u003e}} {{\u003c admonition abstract \u003e}} 一个 **摘要** 横幅 {{\u003c /admonition \u003e}} {{\u003c admonition info \u003e}} 一个 **信息** 横幅 {{\u003c /admonition \u003e}} {{\u003c admonition tip \u003e}} 一个 **技巧** 横幅 {{\u003c /admonition \u003e}} {{\u003c admonition success \u003e}} 一个 **成功** 横幅 {{\u003c /admonition \u003e}} {{\u003c admonition question \u003e}} 一个 **问题** 横幅 {{\u003c /admonition \u003e}} {{\u003c admonition warning \u003e}} 一个 **警告** 横幅 {{\u003c /admonition \u003e}} {{\u003c admonition failure \u003e}} 一个 **失败** 横幅 {{\u003c /admonition \u003e}} {{\u003c admonition danger \u003e}} 一个 **危险** 横幅 {{\u003c /admonition \u003e}} {{\u003c admonition bug \u003e}} 一个 **Bug** 横幅 {{\u003c /admonition \u003e}} {{\u003c admonition example \u003e}} 一个 **示例** 横幅 {{\u003c /admonition \u003e}} {{\u003c admonition quote \u003e}} 一个 **引用** 横幅 {{\u003c /admonition \u003e}} 注意 一个 注意 横幅 摘要 一个 摘要 横幅 信息 一个 信息 横幅 技巧 一个 技巧 横幅 成功 一个 成功 横幅 问题 一个 问题 横幅 警告 一个 警告 横幅 失败 一个 失败 横幅 危险 一个 危险 横幅 Bug 一个 Bug 横幅 示例 一个 示例 横幅 引用 一个 引用 横幅 ","date":"2022-10-29","objectID":"/2022-10-29-my-first-post/:4:0","tags":["test"],"title":"My First Post","uri":"/2022-10-29-my-first-post/"},{"categories":null,"content":"关于 ","date":"0001-01-01","objectID":"/about/:0:0","tags":null,"title":"","uri":"/about/"},{"categories":null,"content":"工作 曾经是 iOS、Java 开发，现在是运维开发，专注 云原生 ","date":"0001-01-01","objectID":"/about/:0:1","tags":null,"title":"","uri":"/about/"},{"categories":null,"content":"参与开源 containerd containerd/containerd #7624 containerd/nerdctl #1618 #1636 #1642 #1775 opencontainers opencontainers/runc #3660 kubesphere kubesphere/kubesphere #5299 Containers containers/podman #16278 kubernetes kubernetes/website #38678 ","date":"0001-01-01","objectID":"/about/:0:2","tags":null,"title":"","uri":"/about/"}]